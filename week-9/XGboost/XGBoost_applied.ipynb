{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38245219347581555"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.5, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 4, \n",
    "                           alpha = 1, \n",
    "                           scale_pos_weight= titanic['Survived'].mean(),\n",
    "                           n_estimators = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=None, n_estimators=10000, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=0.38245219347581555,\n",
       "              seed=None, silent=None, subsample=0.5, verbosity=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.757848\n",
      "F1: 0.635135\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaoalex/opt/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/Users/chaoalex/opt/anaconda3/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "# dmatrix 加速用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 3, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.659772</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.639739</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.640184</td>\n",
       "      <td>0.010529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.627790</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.629019</td>\n",
       "      <td>0.010380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.614868</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>0.616502</td>\n",
       "      <td>0.015193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.599252</td>\n",
       "      <td>0.010671</td>\n",
       "      <td>0.601503</td>\n",
       "      <td>0.014880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.372616</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.434313</td>\n",
       "      <td>0.038028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.371920</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.434456</td>\n",
       "      <td>0.038454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.371195</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.434110</td>\n",
       "      <td>0.038592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.370798</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.434108</td>\n",
       "      <td>0.038613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.370584</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>0.433982</td>\n",
       "      <td>0.038632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.659772           0.000850           0.660168   \n",
       "1              0.639739           0.006761           0.640184   \n",
       "2              0.627790           0.006070           0.629019   \n",
       "3              0.614868           0.010608           0.616502   \n",
       "4              0.599252           0.010671           0.601503   \n",
       "..                  ...                ...                ...   \n",
       "127            0.372616           0.009112           0.434313   \n",
       "128            0.371920           0.009004           0.434456   \n",
       "129            0.371195           0.009250           0.434110   \n",
       "130            0.370798           0.009382           0.434108   \n",
       "131            0.370584           0.009246           0.433982   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001478  \n",
       "1            0.010529  \n",
       "2            0.010380  \n",
       "3            0.015193  \n",
       "4            0.014880  \n",
       "..                ...  \n",
       "127          0.038028  \n",
       "128          0.038454  \n",
       "129          0.038592  \n",
       "130          0.038613  \n",
       "131          0.038632  \n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>876</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8919\n",
      "AUC Score (Train): 0.939281\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "param_dist = {'n_estimators': [100,300,500],\n",
    "              'learning_rate': [0.07,0.05,0.03,0.01],\n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.5,0.45,0.4],\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = clf_xgb,\n",
    "    param_grid = param_dist, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   52.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2700 out of 2700 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.45, 0.4],\n",
       "                         'learning_rate': [0.07, 0.05, 0.03, 0.01],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_child_weight': [1, 2, 3],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.06745811, 0.20969362, 0.30081978, 0.06649027, 0.17317572,\n",
       "        0.2889998 , 0.06327105, 0.18227382, 0.31919746, 0.07023683,\n",
       "        0.23979783, 0.33361082, 0.07021451, 0.22128   , 0.36430106,\n",
       "        0.06867523, 0.19755273, 0.33969979, 0.07793021, 0.24747486,\n",
       "        0.39486222, 0.08157334, 0.22645955, 0.3799232 , 0.08360205,\n",
       "        0.25489955, 0.37762089, 0.09559541, 0.25464559, 0.44666524,\n",
       "        0.10128317, 0.26409373, 0.40559983, 0.08814125, 0.25703931,\n",
       "        0.45398383, 0.12423983, 0.30131574, 0.47796555, 0.12234349,\n",
       "        0.27351952, 0.55257316, 0.13763995, 0.31231666, 0.43205843,\n",
       "        0.06364918, 0.20014005, 0.35720196, 0.07922921, 0.18061104,\n",
       "        0.29763436, 0.06353221, 0.18096347, 0.28709679, 0.07214632,\n",
       "        0.2350461 , 0.33834763, 0.07765398, 0.21217775, 0.34075665,\n",
       "        0.07116122, 0.23478713, 0.33723173, 0.07954645, 0.22656059,\n",
       "        0.41087642, 0.09579301, 0.24909549, 0.38798184, 0.08469028,\n",
       "        0.22112961, 0.54079061, 0.09112997, 0.28384438, 0.44233608,\n",
       "        0.08499079, 0.27289119, 0.41536593, 0.08614936, 0.25186129,\n",
       "        0.42946019, 0.11071458, 0.26921225, 0.45626984, 0.09216256,\n",
       "        0.28666601, 0.43754292, 0.09131694, 0.26136022, 0.46247306,\n",
       "        0.0736516 , 0.19189229, 0.29290962, 0.06571407, 0.17581539,\n",
       "        0.30157719, 0.06793942, 0.20155621, 0.28337722, 0.07365909,\n",
       "        0.21564035, 0.35188575, 0.07100439, 0.24347172, 0.3359292 ,\n",
       "        0.068468  , 0.2035954 , 0.37394152, 0.09702415, 0.25585833,\n",
       "        0.39416456, 0.07999878, 0.22646818, 0.49241648, 0.10014653,\n",
       "        0.23060865, 0.39760656, 0.09328494, 0.27065439, 0.43492589,\n",
       "        0.084934  , 0.24580345, 0.43364177, 0.09811292, 0.26638107,\n",
       "        0.4080471 , 0.0986402 , 0.27415495, 0.48772397, 0.09364748,\n",
       "        0.26531525, 0.45100265, 0.09889898, 0.28122163, 0.43482208,\n",
       "        0.06544509, 0.18087535, 0.3114769 , 0.07498293, 0.18915696,\n",
       "        0.30571856, 0.06713166, 0.20076537, 0.29784284, 0.07151594,\n",
       "        0.24576454, 0.34015713, 0.0718936 , 0.21644087, 0.34558606,\n",
       "        0.07866416, 0.23419695, 0.34191461, 0.08201723, 0.23048005,\n",
       "        0.42001562, 0.11581922, 0.24492331, 0.39383078, 0.09778786,\n",
       "        0.23637171, 0.42891126, 0.08928185, 0.25232038, 0.44314704,\n",
       "        0.08738847, 0.2851438 , 0.41885214, 0.08740363, 0.25049539,\n",
       "        0.43300176, 0.10983381, 0.27542453, 0.46239395, 0.0936439 ,\n",
       "        0.2941927 , 0.45036364, 0.09167776, 0.28036971, 0.50480623,\n",
       "        0.06162024, 0.17907891, 0.29545641, 0.06299601, 0.17395616,\n",
       "        0.35116878, 0.09525828, 0.29912033, 0.32178597, 0.07553539,\n",
       "        0.22329564, 0.40631437, 0.07640061, 0.23690882, 0.38656278,\n",
       "        0.07632098, 0.2762929 , 0.34678254, 0.0856719 , 0.23001966,\n",
       "        0.38500056, 0.10386543, 0.24418893, 0.36825333, 0.07595668,\n",
       "        0.22731562, 0.39969034, 0.09001293, 0.2541996 , 0.40708718,\n",
       "        0.09243026, 0.26951189, 0.42530169, 0.08632684, 0.24201989,\n",
       "        0.41155605, 0.10854626, 0.28624253, 0.45470958, 0.09077144,\n",
       "        0.27169733, 0.46464648, 0.09006634, 0.25692549, 0.42323494,\n",
       "        0.06874337, 0.21051054, 0.30554023, 0.06660137, 0.17632699,\n",
       "        0.30534868, 0.06499286, 0.21602483, 0.29180799, 0.07744923,\n",
       "        0.20771513, 0.34452238, 0.07758627, 0.24366126, 0.34246593,\n",
       "        0.07096066, 0.19863505, 0.33075852, 0.07928348, 0.26235533,\n",
       "        0.37920003, 0.08041353, 0.22771001, 0.38926768, 0.10783892,\n",
       "        0.23514285, 0.37472038, 0.09361186, 0.24869094, 0.44341469,\n",
       "        0.09788542, 0.24408827, 0.40990601, 0.09237261, 0.28276529,\n",
       "        0.42485566, 0.0956687 , 0.27220197, 0.47399707, 0.10223627,\n",
       "        0.26259961, 0.44462461, 0.09014001, 0.26685486, 0.44933147,\n",
       "        0.06335859, 0.1712163 , 0.29583845, 0.06242366, 0.19351201,\n",
       "        0.30549188, 0.06567402, 0.18253484, 0.28865342, 0.07241139,\n",
       "        0.21204553, 0.37829475, 0.07407141, 0.21362181, 0.34410019,\n",
       "        0.07195339, 0.20526648, 0.39539399, 0.08202052, 0.23171978,\n",
       "        0.38873358, 0.08435588, 0.24595904, 0.37073159, 0.08572025,\n",
       "        0.22596569, 0.38471742, 0.09769292, 0.2720397 , 0.42168803,\n",
       "        0.09128952, 0.27601624, 0.46655378, 0.0925703 , 0.26339574,\n",
       "        0.44115968, 0.14632192, 0.30750766, 0.6875916 , 0.09554381,\n",
       "        0.32307959, 0.43222418, 0.08994517, 0.28802333, 0.50344477,\n",
       "        0.08162408, 0.28116589, 0.44699383, 0.09383163, 0.36838245,\n",
       "        0.45187545, 0.09565368, 0.26331115, 0.52577233, 0.09896855,\n",
       "        0.40548162, 0.42865677, 0.0813746 , 0.21710687, 0.38252392,\n",
       "        0.07710247, 0.23922663, 0.38524036, 0.08031859, 0.22849045,\n",
       "        0.39511147, 0.08410883, 0.23828239, 0.43649359, 0.08654404,\n",
       "        0.22528062, 0.39295506, 0.09263701, 0.28484206, 0.43386898,\n",
       "        0.09427381, 0.2583602 , 0.45172987, 0.09393892, 0.25152054,\n",
       "        0.42146044, 0.09841099, 0.3083365 , 0.47122617, 0.08992615,\n",
       "        0.2769701 , 0.48952737, 0.0940124 , 0.2642385 , 0.47299242,\n",
       "        0.05863824, 0.1847086 , 0.28471203, 0.06641278, 0.17013903,\n",
       "        0.27711482, 0.06318336, 0.16640263, 0.30682659, 0.07001038,\n",
       "        0.18701491, 0.31523395, 0.06629181, 0.18352551, 0.34768391,\n",
       "        0.07006288, 0.19847784, 0.301543  , 0.07569637, 0.21852136,\n",
       "        0.3539268 , 0.08359132, 0.21551762, 0.33376098, 0.07000399,\n",
       "        0.19793921, 0.34897962, 0.09638405, 0.2267065 , 0.3709383 ,\n",
       "        0.07615242, 0.20949235, 0.38528261, 0.08067136, 0.21691623,\n",
       "        0.35739484, 0.08389111, 0.23446622, 0.41555181, 0.08120222,\n",
       "        0.22762933, 0.37463517, 0.08552737, 0.23287959, 0.3822475 ,\n",
       "        0.06086659, 0.17103462, 0.27843685, 0.05835805, 0.16748538,\n",
       "        0.30813494, 0.05912385, 0.16315231, 0.27132335, 0.06672635,\n",
       "        0.19365683, 0.33569021, 0.07813983, 0.18566146, 0.3072084 ,\n",
       "        0.06810451, 0.18835049, 0.32177167, 0.08838601, 0.20761709,\n",
       "        0.34076414, 0.07436666, 0.20427909, 0.35128417, 0.08546624,\n",
       "        0.21111283, 0.33363261, 0.07660437, 0.23200679, 0.39510002,\n",
       "        0.08773561, 0.22191691, 0.36517248, 0.07902308, 0.20809302,\n",
       "        0.38752689, 0.0862916 , 0.23181043, 0.39645829, 0.08331332,\n",
       "        0.24504128, 0.39190178, 0.08474202, 0.22558341, 0.37954392,\n",
       "        0.06166816, 0.20121746, 0.2812448 , 0.05570035, 0.16979518,\n",
       "        0.27340217, 0.05681143, 0.17358961, 0.30569925, 0.06786366,\n",
       "        0.18514657, 0.30731502, 0.07332869, 0.1920599 , 0.33313103,\n",
       "        0.06492243, 0.19167142, 0.31008735, 0.0741354 , 0.20501199,\n",
       "        0.38446803, 0.07598414, 0.20739093, 0.36804562, 0.09039392,\n",
       "        0.21212335, 0.35601878, 0.08253627, 0.21636176, 0.3728816 ,\n",
       "        0.07801685, 0.32742252, 0.46812   , 0.08171978, 0.2179842 ,\n",
       "        0.37522459, 0.10344658, 0.24728212, 0.40127668, 0.08943529,\n",
       "        0.24969535, 0.40435338, 0.08036242, 0.23108754, 0.36609435,\n",
       "        0.05917339, 0.18684287, 0.30195675, 0.05855141, 0.1646471 ,\n",
       "        0.28467126, 0.06492066, 0.18168135, 0.31998601, 0.07806053,\n",
       "        0.19616609, 0.31333933, 0.06752329, 0.19196615, 0.34708557,\n",
       "        0.07927685, 0.19682236, 0.32149358, 0.0716332 , 0.20476995,\n",
       "        0.35904083, 0.08532715, 0.21696448, 0.34583731, 0.07004175,\n",
       "        0.20524402, 0.36462722, 0.09183445, 0.23014097, 0.37228189,\n",
       "        0.0788341 , 0.22076879, 0.40079637, 0.07702765, 0.21874676,\n",
       "        0.35676055, 0.08044238, 0.24416203, 0.42071495, 0.08426905,\n",
       "        0.22623816, 0.38312755, 0.07928982, 0.25629959, 0.357025  ]),\n",
       " 'std_fit_time': array([0.00807573, 0.02124864, 0.00658633, 0.00407875, 0.00336334,\n",
       "        0.01983851, 0.00205714, 0.01755558, 0.01863037, 0.00557713,\n",
       "        0.01535201, 0.00538827, 0.00329141, 0.03459489, 0.02355918,\n",
       "        0.00165917, 0.00392611, 0.00734691, 0.00133517, 0.03128407,\n",
       "        0.00344554, 0.00134621, 0.00415571, 0.02524271, 0.00380137,\n",
       "        0.01494448, 0.01639605, 0.00523524, 0.0047268 , 0.01968331,\n",
       "        0.01155848, 0.01447379, 0.00511726, 0.00577454, 0.02230102,\n",
       "        0.02816284, 0.01669352, 0.01685801, 0.03928116, 0.01892661,\n",
       "        0.01527901, 0.09632844, 0.02307284, 0.01091216, 0.01359964,\n",
       "        0.00318114, 0.02956526, 0.02980279, 0.01128834, 0.0087057 ,\n",
       "        0.00811541, 0.00240995, 0.00399085, 0.00632834, 0.00409548,\n",
       "        0.01589842, 0.00755011, 0.00862171, 0.01007978, 0.00438298,\n",
       "        0.00252294, 0.01313825, 0.0066269 , 0.00223319, 0.00303023,\n",
       "        0.01971451, 0.01445542, 0.00692359, 0.01049787, 0.00289151,\n",
       "        0.00267391, 0.07224851, 0.00419207, 0.01146121, 0.01034675,\n",
       "        0.00180121, 0.0130825 , 0.00666459, 0.00464792, 0.00562216,\n",
       "        0.01627863, 0.01248636, 0.00561045, 0.00844535, 0.00281077,\n",
       "        0.0289505 , 0.02129531, 0.00258209, 0.01418147, 0.0187142 ,\n",
       "        0.01366114, 0.00915382, 0.00631681, 0.00211713, 0.00568162,\n",
       "        0.0157511 , 0.0057153 , 0.01897993, 0.01098294, 0.00457868,\n",
       "        0.01199213, 0.01124124, 0.00145343, 0.03002144, 0.00838367,\n",
       "        0.00104199, 0.00716077, 0.0189404 , 0.00761767, 0.01375104,\n",
       "        0.00826119, 0.00199968, 0.00561253, 0.0766215 , 0.02402027,\n",
       "        0.0031933 , 0.01021562, 0.00303416, 0.02166037, 0.01776095,\n",
       "        0.00196714, 0.0041448 , 0.0219595 , 0.01793088, 0.01442438,\n",
       "        0.00641227, 0.00252203, 0.01513651, 0.01727608, 0.00127446,\n",
       "        0.00470679, 0.01959348, 0.00845354, 0.01572383, 0.0103951 ,\n",
       "        0.00207218, 0.0152089 , 0.0271785 , 0.01257251, 0.0101918 ,\n",
       "        0.00817588, 0.00460003, 0.00947967, 0.02014599, 0.0038504 ,\n",
       "        0.0208598 , 0.00937267, 0.00268251, 0.00602802, 0.02630522,\n",
       "        0.01010753, 0.01187211, 0.00751755, 0.00237806, 0.00514107,\n",
       "        0.04325332, 0.01939494, 0.01943433, 0.01617394, 0.01725203,\n",
       "        0.01459165, 0.0250437 , 0.00433512, 0.0034653 , 0.01111762,\n",
       "        0.00196545, 0.02736227, 0.00987548, 0.00706953, 0.0050365 ,\n",
       "        0.02449155, 0.01474076, 0.00759681, 0.00843047, 0.00144829,\n",
       "        0.01619108, 0.00585007, 0.00360147, 0.03703028, 0.02441327,\n",
       "        0.00213734, 0.00501014, 0.01246983, 0.00294971, 0.00389725,\n",
       "        0.05960599, 0.02336321, 0.05088236, 0.01927906, 0.00804222,\n",
       "        0.00970933, 0.02735616, 0.00352174, 0.04477142, 0.04408752,\n",
       "        0.00148391, 0.02767031, 0.01321405, 0.00532707, 0.00257539,\n",
       "        0.01338052, 0.02352065, 0.00666633, 0.00807539, 0.003032  ,\n",
       "        0.00887456, 0.02596278, 0.00526001, 0.00408661, 0.00866203,\n",
       "        0.00392002, 0.03187223, 0.02958694, 0.00301837, 0.00704659,\n",
       "        0.0199023 , 0.01479266, 0.0125701 , 0.00402439, 0.00154554,\n",
       "        0.00439941, 0.01529765, 0.00230446, 0.0118072 , 0.02833096,\n",
       "        0.0081263 , 0.0097644 , 0.00655797, 0.00205636, 0.00259562,\n",
       "        0.00803689, 0.00185199, 0.01020479, 0.01453872, 0.00407254,\n",
       "        0.0034818 , 0.01014247, 0.00614773, 0.01620195, 0.01391473,\n",
       "        0.00243363, 0.00572302, 0.01072205, 0.00243268, 0.01259904,\n",
       "        0.0051901 , 0.00321082, 0.00459214, 0.03024217, 0.02555264,\n",
       "        0.02116622, 0.00459441, 0.00275359, 0.0034745 , 0.03010364,\n",
       "        0.00809595, 0.00285746, 0.00740587, 0.00631052, 0.0192277 ,\n",
       "        0.01341902, 0.00138711, 0.00361702, 0.02738982, 0.0060264 ,\n",
       "        0.00643882, 0.01756211, 0.00328594, 0.020365  , 0.01016625,\n",
       "        0.00308245, 0.00500977, 0.00487364, 0.00205851, 0.0171875 ,\n",
       "        0.01432821, 0.00469176, 0.00301903, 0.00297476, 0.00206523,\n",
       "        0.00872243, 0.0246684 , 0.00731447, 0.00736168, 0.00947048,\n",
       "        0.00178771, 0.01284452, 0.02878467, 0.00319286, 0.0032997 ,\n",
       "        0.01448551, 0.00264818, 0.01242836, 0.00375195, 0.00543633,\n",
       "        0.00358359, 0.018303  , 0.01473486, 0.00864672, 0.0155117 ,\n",
       "        0.00466914, 0.01765084, 0.01659772, 0.00364069, 0.00598476,\n",
       "        0.05374675, 0.03139949, 0.02389169, 0.09580526, 0.0026243 ,\n",
       "        0.01865574, 0.00707932, 0.00229169, 0.01007357, 0.04501863,\n",
       "        0.01220905, 0.02054852, 0.03262269, 0.00473988, 0.05535986,\n",
       "        0.0118947 , 0.00759783, 0.01362835, 0.03092844, 0.02125208,\n",
       "        0.0517423 , 0.01082853, 0.01916203, 0.00908709, 0.00568871,\n",
       "        0.00527691, 0.02212156, 0.04396573, 0.00057774, 0.00597856,\n",
       "        0.01384099, 0.00146883, 0.01724366, 0.01558677, 0.00246561,\n",
       "        0.00330343, 0.01994886, 0.00304676, 0.02507582, 0.02025822,\n",
       "        0.0071008 , 0.00801957, 0.02910057, 0.00541131, 0.00893111,\n",
       "        0.00749997, 0.0019277 , 0.0157152 , 0.00702177, 0.00169825,\n",
       "        0.01770016, 0.02011195, 0.00240004, 0.02475579, 0.02938424,\n",
       "        0.00120152, 0.00800434, 0.0116674 , 0.01112644, 0.00367251,\n",
       "        0.01019428, 0.00416109, 0.00348537, 0.0280027 , 0.00573835,\n",
       "        0.00555248, 0.00310129, 0.00223124, 0.0049878 , 0.02909814,\n",
       "        0.00816919, 0.00851804, 0.00304924, 0.00540163, 0.01063078,\n",
       "        0.02363931, 0.01128643, 0.00853636, 0.01019738, 0.00257188,\n",
       "        0.009171  , 0.0147533 , 0.0084361 , 0.00511308, 0.01525629,\n",
       "        0.00146947, 0.00376167, 0.02027222, 0.00666338, 0.00446645,\n",
       "        0.02058843, 0.00128529, 0.01026207, 0.01346814, 0.00406035,\n",
       "        0.00151589, 0.01035161, 0.0069344 , 0.01167916, 0.02025304,\n",
       "        0.0042184 , 0.00681252, 0.0041225 , 0.00290575, 0.00864137,\n",
       "        0.0181782 , 0.00200275, 0.00212633, 0.00765385, 0.00285183,\n",
       "        0.00411001, 0.0291456 , 0.01124172, 0.00646919, 0.00613495,\n",
       "        0.00388631, 0.00408095, 0.03093321, 0.01612465, 0.00582487,\n",
       "        0.00671025, 0.00262315, 0.0040482 , 0.02695689, 0.00705254,\n",
       "        0.00555344, 0.00899907, 0.00146964, 0.01353596, 0.01641799,\n",
       "        0.00693903, 0.01876319, 0.00820091, 0.00525666, 0.00751616,\n",
       "        0.00878761, 0.00581389, 0.00675003, 0.01025533, 0.00207496,\n",
       "        0.01457615, 0.01120044, 0.00571522, 0.01004147, 0.02757838,\n",
       "        0.00328738, 0.0138199 , 0.0081224 , 0.00061857, 0.00239416,\n",
       "        0.00583095, 0.00130867, 0.01347399, 0.01739401, 0.00202456,\n",
       "        0.0057945 , 0.01105696, 0.00486565, 0.01230932, 0.01653045,\n",
       "        0.00395948, 0.00468719, 0.0032239 , 0.00196387, 0.00254305,\n",
       "        0.01985722, 0.00386775, 0.00226958, 0.03711994, 0.01540959,\n",
       "        0.00422206, 0.01114516, 0.00377918, 0.00601709, 0.01212041,\n",
       "        0.00155061, 0.06107122, 0.05812971, 0.0070083 , 0.00637184,\n",
       "        0.03536122, 0.00994636, 0.00710552, 0.02499467, 0.01445992,\n",
       "        0.02164355, 0.00882143, 0.00476341, 0.00896349, 0.00946412,\n",
       "        0.00132116, 0.01538746, 0.02184465, 0.00215524, 0.00239058,\n",
       "        0.01480797, 0.0035582 , 0.00654908, 0.02164789, 0.00360594,\n",
       "        0.00835175, 0.01111621, 0.0031246 , 0.01043174, 0.02241095,\n",
       "        0.01099055, 0.00544838, 0.00754293, 0.00119377, 0.0054251 ,\n",
       "        0.02957696, 0.01316179, 0.00598902, 0.01876199, 0.00348501,\n",
       "        0.00435005, 0.02644378, 0.01235693, 0.00201689, 0.00514091,\n",
       "        0.00609928, 0.00661818, 0.01049505, 0.00317832, 0.0060388 ,\n",
       "        0.01262115, 0.00165299, 0.02550726, 0.00860591, 0.00564021,\n",
       "        0.00696747, 0.00706957, 0.0033976 , 0.00614676, 0.06329936]),\n",
       " 'mean_score_time': array([0.00420389, 0.00537653, 0.00671067, 0.00396705, 0.00468764,\n",
       "        0.00760398, 0.00409546, 0.00667286, 0.00649419, 0.00901585,\n",
       "        0.00559649, 0.00711484, 0.00387497, 0.00560503, 0.00812416,\n",
       "        0.00407495, 0.00586042, 0.00764732, 0.00401678, 0.00652018,\n",
       "        0.00983944, 0.00398378, 0.00563254, 0.00872731, 0.0051301 ,\n",
       "        0.00597062, 0.00922065, 0.00532918, 0.00640016, 0.00877542,\n",
       "        0.00442157, 0.00630956, 0.0089323 , 0.00403633, 0.0068881 ,\n",
       "        0.00841279, 0.0045238 , 0.00739045, 0.01103668, 0.01039853,\n",
       "        0.0074779 , 0.0122448 , 0.00619521, 0.00853338, 0.00814414,\n",
       "        0.00398088, 0.00919218, 0.00717454, 0.00396647, 0.00480461,\n",
       "        0.00682602, 0.00419149, 0.00565529, 0.00607347, 0.00450869,\n",
       "        0.00568857, 0.0073082 , 0.00557246, 0.00653844, 0.00748773,\n",
       "        0.00393877, 0.00598063, 0.00766506, 0.00496702, 0.0067122 ,\n",
       "        0.01091933, 0.00619946, 0.00603609, 0.00809622, 0.00414472,\n",
       "        0.00556893, 0.01351781, 0.0041728 , 0.00776057, 0.00897794,\n",
       "        0.00427299, 0.00757103, 0.00900292, 0.00420036, 0.00671916,\n",
       "        0.01036916, 0.00451736, 0.00683441, 0.00933051, 0.00452328,\n",
       "        0.00786262, 0.00845118, 0.00408549, 0.00669222, 0.0102572 ,\n",
       "        0.00457006, 0.00570765, 0.00613599, 0.00383625, 0.00477424,\n",
       "        0.00708976, 0.00391216, 0.00510907, 0.00714359, 0.00393462,\n",
       "        0.00624809, 0.00741758, 0.00391212, 0.00592408, 0.00829015,\n",
       "        0.00395069, 0.00732002, 0.00795317, 0.00463586, 0.00744939,\n",
       "        0.008566  , 0.00426025, 0.00595312, 0.01625719, 0.00437503,\n",
       "        0.00680842, 0.00814214, 0.00428371, 0.00693798, 0.00936422,\n",
       "        0.0054121 , 0.00652046, 0.00899758, 0.00512481, 0.00840578,\n",
       "        0.00859456, 0.00427389, 0.00700183, 0.0095479 , 0.00427823,\n",
       "        0.00678968, 0.00994515, 0.00495601, 0.00739384, 0.0090106 ,\n",
       "        0.00367341, 0.00500784, 0.00854807, 0.00483127, 0.00567365,\n",
       "        0.00655684, 0.00430331, 0.00502038, 0.00668721, 0.00393682,\n",
       "        0.00552597, 0.00742388, 0.00413523, 0.00622473, 0.0090766 ,\n",
       "        0.0057518 , 0.00537062, 0.00737038, 0.00410085, 0.00596452,\n",
       "        0.00944295, 0.00497742, 0.00587602, 0.00780363, 0.00645504,\n",
       "        0.0057538 , 0.00792313, 0.00406218, 0.00636401, 0.01002998,\n",
       "        0.00405073, 0.00712743, 0.00866265, 0.00407043, 0.0068656 ,\n",
       "        0.00883217, 0.00436177, 0.00712867, 0.00946074, 0.00410957,\n",
       "        0.00770888, 0.0089828 , 0.00408516, 0.00727224, 0.01133404,\n",
       "        0.00812054, 0.00509262, 0.00613046, 0.0042757 , 0.00560656,\n",
       "        0.0093452 , 0.00385928, 0.00776682, 0.01578159, 0.00437598,\n",
       "        0.00811443, 0.00813479, 0.00513582, 0.00917907, 0.00854139,\n",
       "        0.00445108, 0.00638218, 0.00751786, 0.00524659, 0.0059144 ,\n",
       "        0.00880384, 0.0078908 , 0.0072639 , 0.00767245, 0.00398703,\n",
       "        0.00737658, 0.00839901, 0.00423055, 0.00694585, 0.00851278,\n",
       "        0.00423837, 0.00627975, 0.00963011, 0.0046371 , 0.00601058,\n",
       "        0.00897117, 0.00513515, 0.0071157 , 0.00960197, 0.0043839 ,\n",
       "        0.00856447, 0.00978942, 0.00499024, 0.0060545 , 0.00870204,\n",
       "        0.00411696, 0.00488853, 0.00659776, 0.00378828, 0.00520053,\n",
       "        0.00698614, 0.0045845 , 0.00555115, 0.00661697, 0.00451827,\n",
       "        0.00589395, 0.00706005, 0.00412035, 0.00602155, 0.00789833,\n",
       "        0.00378575, 0.00609164, 0.00707607, 0.0039443 , 0.00716848,\n",
       "        0.00876884, 0.00460944, 0.00675654, 0.0080296 , 0.00615482,\n",
       "        0.00556479, 0.00814977, 0.006002  , 0.00618644, 0.00955143,\n",
       "        0.00533876, 0.00657711, 0.00885482, 0.00445919, 0.00664597,\n",
       "        0.00904207, 0.00507975, 0.00686078, 0.00942893, 0.0078793 ,\n",
       "        0.00678444, 0.00887952, 0.00450602, 0.00661764, 0.00905371,\n",
       "        0.00486121, 0.00486608, 0.008214  , 0.00492363, 0.00691657,\n",
       "        0.00667076, 0.0044476 , 0.00537009, 0.00775018, 0.00398726,\n",
       "        0.00584545, 0.00907431, 0.00447598, 0.00709682, 0.00747452,\n",
       "        0.00457921, 0.00818653, 0.00766525, 0.00425954, 0.00617542,\n",
       "        0.00954266, 0.00411119, 0.00578203, 0.00986485, 0.00419168,\n",
       "        0.00591011, 0.00781307, 0.00599432, 0.00644631, 0.01202335,\n",
       "        0.00433302, 0.00686936, 0.00990405, 0.0049201 , 0.00641441,\n",
       "        0.01061206, 0.00500998, 0.0068821 , 0.01402302, 0.00422444,\n",
       "        0.00682979, 0.00851121, 0.00404048, 0.00632048, 0.01189499,\n",
       "        0.00428123, 0.00815067, 0.0089467 , 0.00671363, 0.01063237,\n",
       "        0.01112409, 0.00639486, 0.00913315, 0.01061759, 0.01320319,\n",
       "        0.00864024, 0.0111156 , 0.00383787, 0.00668864, 0.00825753,\n",
       "        0.00425501, 0.0094656 , 0.00713944, 0.005023  , 0.00587969,\n",
       "        0.00831852, 0.00424037, 0.00704889, 0.00820022, 0.00419617,\n",
       "        0.00576115, 0.0095686 , 0.00425887, 0.00688257, 0.00939684,\n",
       "        0.00441117, 0.0062737 , 0.00975356, 0.00449824, 0.00593143,\n",
       "        0.00845351, 0.0045351 , 0.00677795, 0.00920916, 0.00410457,\n",
       "        0.00689526, 0.01018124, 0.00480218, 0.00683093, 0.00899882,\n",
       "        0.00412102, 0.0069818 , 0.00677657, 0.00423779, 0.00535164,\n",
       "        0.00630231, 0.00397739, 0.00577612, 0.00679865, 0.00410213,\n",
       "        0.00551991, 0.00731149, 0.00423741, 0.00500727, 0.00855365,\n",
       "        0.00383639, 0.00550494, 0.00672874, 0.00445895, 0.00615268,\n",
       "        0.00916686, 0.00427804, 0.00622058, 0.00773401, 0.00418653,\n",
       "        0.00564904, 0.00750575, 0.00736136, 0.00644021, 0.00866103,\n",
       "        0.00475903, 0.00579867, 0.01144013, 0.00422726, 0.00695434,\n",
       "        0.00868969, 0.00436263, 0.0064868 , 0.00984902, 0.0043684 ,\n",
       "        0.00637069, 0.00919452, 0.00463896, 0.00689526, 0.00871024,\n",
       "        0.00395617, 0.00584183, 0.00629501, 0.00390797, 0.00511303,\n",
       "        0.00715742, 0.00434718, 0.00523825, 0.00684781, 0.00410399,\n",
       "        0.0061934 , 0.00920897, 0.00414   , 0.00542626, 0.00746498,\n",
       "        0.00459361, 0.00516143, 0.00760603, 0.00509605, 0.00648718,\n",
       "        0.00842838, 0.0045886 , 0.00633869, 0.00801067, 0.00451474,\n",
       "        0.00642943, 0.00992098, 0.00428052, 0.00834503, 0.01317701,\n",
       "        0.00619879, 0.00698624, 0.00826955, 0.00419006, 0.00604277,\n",
       "        0.00917706, 0.0043025 , 0.00693464, 0.01023154, 0.00463595,\n",
       "        0.00651307, 0.00953302, 0.00450802, 0.00650043, 0.00918512,\n",
       "        0.00427904, 0.00572071, 0.00695658, 0.00384145, 0.00573378,\n",
       "        0.00676975, 0.00373459, 0.00809412, 0.00615244, 0.00465031,\n",
       "        0.00549302, 0.00805783, 0.00418134, 0.00611668, 0.00903921,\n",
       "        0.00392604, 0.00611877, 0.00811963, 0.00400553, 0.00617948,\n",
       "        0.00970483, 0.00433683, 0.00554242, 0.00974059, 0.00563493,\n",
       "        0.00577202, 0.00864196, 0.00426545, 0.00609713, 0.00883803,\n",
       "        0.00517192, 0.01162729, 0.00991321, 0.00423665, 0.00617623,\n",
       "        0.00816145, 0.00673838, 0.0073936 , 0.00932345, 0.00777755,\n",
       "        0.00597582, 0.00855598, 0.00436182, 0.00641041, 0.0099659 ,\n",
       "        0.00447488, 0.00547957, 0.00628209, 0.00496483, 0.00525126,\n",
       "        0.00805478, 0.00401263, 0.00579548, 0.01189275, 0.00435505,\n",
       "        0.0057929 , 0.00787377, 0.00485649, 0.00652375, 0.0080287 ,\n",
       "        0.00415606, 0.0065587 , 0.00707903, 0.00400205, 0.00512099,\n",
       "        0.00994616, 0.00700769, 0.00706143, 0.00787153, 0.00440817,\n",
       "        0.00655088, 0.0125608 , 0.00444322, 0.00637307, 0.00874243,\n",
       "        0.00428019, 0.006532  , 0.00845203, 0.00401225, 0.00596404,\n",
       "        0.007934  , 0.00429707, 0.00669212, 0.00995517, 0.00449214,\n",
       "        0.00620112, 0.00882316, 0.0042491 , 0.00627756, 0.00722313]),\n",
       " 'std_score_time': array([9.97230203e-04, 6.08795302e-04, 6.80969635e-04, 1.73032569e-04,\n",
       "        1.45101289e-04, 2.20335698e-03, 3.04272163e-04, 3.59845820e-03,\n",
       "        1.38389249e-03, 4.32839811e-03, 2.44081460e-04, 1.85717355e-04,\n",
       "        2.67341227e-04, 2.35342949e-04, 2.24791492e-03, 4.71978001e-04,\n",
       "        3.81840544e-04, 1.49627111e-03, 1.42294760e-04, 1.08577930e-03,\n",
       "        1.98674871e-03, 1.78824831e-04, 2.05959962e-04, 1.03095268e-03,\n",
       "        1.07900865e-03, 5.45793402e-05, 1.90033787e-03, 1.71320050e-03,\n",
       "        2.97268767e-04, 6.48094595e-04, 3.17290383e-04, 4.66783306e-04,\n",
       "        4.09115872e-04, 1.64902162e-04, 1.20143423e-03, 5.19275127e-04,\n",
       "        3.30423496e-04, 7.21093867e-04, 1.93479671e-03, 9.21264715e-03,\n",
       "        9.82392680e-04, 2.27332700e-03, 3.34796923e-03, 3.83871682e-03,\n",
       "        1.27625267e-04, 4.53995529e-04, 4.28453027e-03, 1.59944852e-03,\n",
       "        5.32375962e-04, 1.81454490e-04, 6.19014428e-04, 6.79308090e-04,\n",
       "        9.65911307e-04, 2.63323234e-04, 1.15763456e-03, 2.71472804e-04,\n",
       "        7.11928257e-05, 3.15332772e-03, 1.90208125e-03, 1.41130540e-04,\n",
       "        2.34430800e-04, 1.45690710e-03, 8.71769305e-04, 1.72601914e-03,\n",
       "        1.26354047e-03, 5.06325901e-03, 3.84527936e-03, 4.55679106e-04,\n",
       "        6.66587871e-04, 1.70510020e-04, 1.26226947e-04, 1.06884290e-02,\n",
       "        3.62457879e-04, 1.46325303e-03, 1.20997432e-03, 2.40176368e-04,\n",
       "        9.03328151e-04, 6.09387945e-04, 4.41398594e-04, 6.69685907e-04,\n",
       "        1.78488991e-03, 2.98483166e-04, 1.20787342e-04, 5.32721419e-04,\n",
       "        4.69565280e-04, 1.78445552e-03, 3.50714213e-04, 1.51018130e-04,\n",
       "        5.54703365e-04, 3.90427339e-03, 9.68858789e-04, 9.12508801e-04,\n",
       "        3.69599751e-04, 1.98760611e-04, 2.85187275e-04, 5.56789594e-04,\n",
       "        1.65997035e-04, 9.77252552e-04, 1.16950930e-03, 2.02054140e-04,\n",
       "        1.02460450e-03, 1.29385227e-04, 2.39655889e-04, 7.94668840e-04,\n",
       "        2.53089897e-03, 1.78559478e-04, 3.22427538e-03, 8.09777674e-04,\n",
       "        6.01278315e-04, 1.94970756e-03, 5.68695066e-04, 5.47179093e-04,\n",
       "        2.43837962e-04, 1.17680554e-02, 5.52067544e-04, 7.00072718e-04,\n",
       "        8.70153589e-04, 1.79138487e-04, 6.61489965e-04, 1.48606919e-03,\n",
       "        2.18539015e-03, 2.36549413e-04, 1.26544419e-03, 2.15113843e-03,\n",
       "        3.78835703e-03, 7.25469436e-04, 3.03107682e-04, 6.62202758e-04,\n",
       "        7.18786512e-04, 1.44719048e-04, 2.09378974e-04, 1.71693949e-03,\n",
       "        8.22657909e-04, 8.22060789e-04, 6.68166124e-04, 3.04758733e-04,\n",
       "        6.64120173e-04, 3.42937649e-03, 1.70942272e-03, 1.85196719e-03,\n",
       "        5.57519213e-04, 1.21217088e-03, 3.37385659e-04, 1.46694637e-03,\n",
       "        3.96043083e-04, 1.04758487e-04, 4.86301061e-04, 2.32637097e-04,\n",
       "        9.68675276e-04, 1.89385868e-03, 3.23968496e-03, 3.01571626e-04,\n",
       "        6.53017696e-04, 2.69907498e-04, 1.08511103e-04, 1.59497055e-03,\n",
       "        6.70320565e-04, 3.23485970e-04, 3.13299591e-04, 3.77482358e-03,\n",
       "        2.16644675e-04, 5.15349223e-04, 1.76339449e-04, 3.71359071e-04,\n",
       "        1.45106142e-03, 4.68313059e-04, 8.98351697e-04, 6.03596296e-04,\n",
       "        2.19519877e-04, 1.41411891e-03, 7.05816617e-04, 1.63255116e-04,\n",
       "        5.12854522e-04, 1.12917175e-03, 1.47847927e-04, 1.47396336e-03,\n",
       "        1.11821749e-03, 1.69312084e-04, 2.05918257e-03, 2.54348194e-03,\n",
       "        7.77052207e-03, 4.34433981e-04, 2.98282892e-04, 4.50574142e-04,\n",
       "        1.59804986e-03, 3.65257857e-03, 2.83285380e-04, 3.60095651e-03,\n",
       "        1.10385016e-02, 6.67351455e-04, 3.46514926e-03, 5.81975686e-04,\n",
       "        2.05385116e-03, 5.24021569e-03, 1.23564103e-03, 4.18233717e-04,\n",
       "        9.55936388e-04, 5.78728920e-04, 1.50099726e-03, 2.66306974e-04,\n",
       "        9.89518277e-04, 7.40814298e-03, 2.00806388e-03, 2.12712914e-04,\n",
       "        8.72022138e-05, 1.82757600e-03, 1.60520935e-03, 1.81642627e-04,\n",
       "        5.44489158e-04, 3.64045806e-04, 4.53262150e-04, 2.78594467e-04,\n",
       "        1.36375524e-03, 1.02939353e-03, 5.43360898e-04, 1.39103843e-03,\n",
       "        9.87625124e-04, 9.51801805e-04, 7.84780988e-04, 4.58922053e-04,\n",
       "        2.67253527e-03, 1.33203711e-03, 1.61401647e-03, 1.19016071e-04,\n",
       "        6.92332233e-04, 7.46840624e-04, 3.67753220e-04, 8.13464948e-04,\n",
       "        8.04631303e-05, 6.06423842e-04, 3.88023787e-04, 1.02098664e-03,\n",
       "        1.00851500e-03, 5.36980689e-04, 3.71908591e-04, 6.08462674e-04,\n",
       "        3.45450964e-04, 2.91465128e-04, 5.44507832e-04, 1.02829663e-03,\n",
       "        6.94494291e-05, 6.19037418e-04, 3.87133614e-04, 1.41003304e-04,\n",
       "        2.06023143e-03, 8.29221318e-04, 5.96280521e-04, 2.18519551e-03,\n",
       "        3.52544470e-04, 2.69146029e-03, 1.25339493e-04, 4.60979930e-04,\n",
       "        3.22730689e-03, 5.01802872e-04, 6.20208302e-04, 8.74304298e-04,\n",
       "        8.01762789e-04, 5.46943661e-04, 2.23097497e-04, 7.87519216e-04,\n",
       "        1.51430635e-03, 9.24896245e-04, 1.69093435e-04, 6.33616928e-04,\n",
       "        5.00377779e-03, 6.80693870e-04, 6.14691900e-04, 6.96739643e-04,\n",
       "        2.74017434e-04, 7.24474028e-04, 1.85672821e-03, 1.94755520e-04,\n",
       "        1.40245883e-03, 1.17241446e-03, 2.99298052e-03, 2.73773902e-04,\n",
       "        5.73976567e-04, 3.04820830e-04, 1.52581986e-03, 1.70407577e-04,\n",
       "        3.13422151e-04, 3.50677051e-03, 3.23991624e-04, 1.84680389e-03,\n",
       "        2.51690923e-04, 9.76466553e-04, 5.42131955e-03, 5.85906828e-04,\n",
       "        1.64970524e-04, 3.96498501e-04, 9.67772788e-04, 2.56264001e-04,\n",
       "        1.86249237e-04, 1.80468684e-03, 3.46240073e-04, 1.74154052e-04,\n",
       "        5.35512089e-04, 3.41192123e-03, 2.35029954e-04, 5.55930156e-03,\n",
       "        3.21359095e-04, 7.18812419e-04, 1.26971359e-03, 8.17364356e-04,\n",
       "        5.26264068e-04, 3.93838134e-03, 9.33958224e-04, 2.35255652e-04,\n",
       "        6.91501557e-03, 3.14169422e-04, 4.64412157e-04, 2.05323810e-04,\n",
       "        9.40879585e-05, 4.10645893e-04, 4.36226885e-03, 5.60820898e-04,\n",
       "        3.87973823e-03, 4.43252015e-03, 3.74413972e-03, 4.48518962e-03,\n",
       "        5.53958242e-03, 2.73036966e-03, 3.28252284e-03, 5.41677381e-03,\n",
       "        1.75911907e-02, 4.59931729e-03, 6.44077293e-03, 7.65507667e-05,\n",
       "        1.20946264e-03, 9.32417962e-04, 5.37597416e-04, 8.27781470e-03,\n",
       "        3.23252584e-04, 2.39181249e-03, 1.57612707e-04, 3.02058654e-04,\n",
       "        3.31143670e-04, 8.65820337e-04, 4.35069327e-04, 4.61925448e-04,\n",
       "        1.56253141e-04, 1.74865555e-03, 2.12654618e-04, 1.11710904e-03,\n",
       "        1.35934301e-03, 3.28976808e-04, 4.77678472e-04, 1.04191420e-03,\n",
       "        3.83233968e-04, 1.06895299e-04, 2.88893460e-04, 4.44358523e-04,\n",
       "        4.15527002e-04, 5.68670565e-04, 8.88338028e-05, 3.28825687e-04,\n",
       "        2.27747793e-03, 1.17753108e-03, 1.34268200e-03, 4.45770584e-04,\n",
       "        4.68376133e-04, 3.52454503e-03, 1.51985183e-03, 8.35635286e-04,\n",
       "        4.03187758e-04, 2.45572775e-04, 4.55059308e-04, 1.97612522e-03,\n",
       "        3.76861351e-04, 1.92010701e-04, 2.76124018e-04, 5.30470004e-04,\n",
       "        7.79900256e-04, 7.98705798e-05, 3.01474238e-03, 5.19052590e-04,\n",
       "        1.57899108e-04, 2.43803188e-04, 4.79457033e-04, 2.35881342e-04,\n",
       "        1.28301122e-03, 7.49125147e-04, 8.35097863e-04, 5.10207724e-04,\n",
       "        1.12200462e-04, 2.69498702e-04, 1.79673519e-04, 5.78083539e-03,\n",
       "        3.62620610e-04, 4.46204814e-04, 9.98643459e-04, 1.58134372e-04,\n",
       "        4.36918482e-03, 4.03555758e-04, 3.23549019e-04, 1.24079442e-03,\n",
       "        1.01276373e-04, 4.03481492e-04, 1.35860142e-03, 5.58022066e-04,\n",
       "        6.14706973e-04, 1.16208495e-03, 7.69669257e-04, 2.30249844e-03,\n",
       "        5.53505204e-04, 1.81759829e-04, 1.32666520e-03, 4.73834200e-04,\n",
       "        2.01974306e-04, 2.67379335e-04, 1.02060334e-03, 6.53795436e-04,\n",
       "        1.00271454e-03, 1.32506996e-03, 3.91777534e-04, 1.05705607e-03,\n",
       "        3.74871045e-03, 4.12128251e-04, 3.97171853e-04, 3.24227550e-04,\n",
       "        6.07707342e-04, 2.22652860e-04, 4.02686566e-04, 1.12750342e-03,\n",
       "        1.28370162e-03, 8.68254585e-04, 4.86453672e-04, 1.22681764e-03,\n",
       "        9.98782847e-04, 4.22277721e-04, 7.81362004e-04, 3.03332127e-03,\n",
       "        3.31077053e-04, 3.62122897e-03, 4.05457260e-03, 2.21385484e-03,\n",
       "        7.78049576e-04, 6.37416733e-04, 3.37572507e-04, 5.95664692e-04,\n",
       "        3.01951637e-03, 3.68561503e-04, 5.97330751e-04, 2.64534091e-03,\n",
       "        3.69960451e-04, 3.36747702e-04, 8.43506827e-04, 3.43193785e-04,\n",
       "        7.31707488e-04, 1.01504958e-03, 3.75340654e-04, 8.00668951e-04,\n",
       "        1.43995099e-03, 1.40349256e-04, 2.06490806e-03, 4.24917043e-04,\n",
       "        2.61464080e-04, 4.23340342e-03, 2.41904026e-04, 1.24383855e-03,\n",
       "        3.72609739e-04, 1.48224717e-03, 4.28046824e-04, 5.56028735e-04,\n",
       "        1.92568532e-03, 2.89834907e-04, 7.51066645e-04, 2.01425130e-03,\n",
       "        1.12568074e-04, 7.08234123e-04, 2.13367418e-03, 3.72163750e-04,\n",
       "        1.36754420e-04, 3.11157756e-03, 1.43920068e-03, 3.87593194e-04,\n",
       "        1.36725875e-03, 2.86378393e-04, 2.27328103e-04, 3.49733957e-04,\n",
       "        1.81039064e-03, 7.01774269e-03, 2.44167055e-03, 2.01317429e-04,\n",
       "        6.86418989e-04, 2.81383567e-04, 4.19763678e-03, 7.13022130e-04,\n",
       "        6.60289734e-04, 6.51262531e-03, 4.68950414e-04, 3.15560679e-04,\n",
       "        6.18729671e-04, 5.77943482e-04, 4.12812641e-03, 7.89846418e-04,\n",
       "        8.12885368e-04, 4.39115168e-04, 1.09959888e-03, 7.81835946e-04,\n",
       "        2.19086411e-03, 1.54178598e-04, 4.71454351e-04, 8.71396495e-03,\n",
       "        1.85447212e-04, 5.23880945e-04, 1.16106991e-03, 1.42151744e-03,\n",
       "        1.73747742e-03, 1.22786338e-03, 4.14253263e-04, 9.19331498e-04,\n",
       "        3.77253496e-04, 2.43512824e-04, 6.13578799e-04, 3.83485362e-03,\n",
       "        5.07862899e-03, 1.99448465e-03, 2.03715393e-04, 4.22738809e-04,\n",
       "        9.97500079e-04, 8.42083185e-03, 4.96746089e-04, 4.58601681e-04,\n",
       "        8.46892066e-04, 1.33866510e-04, 2.99819659e-04, 7.00553318e-04,\n",
       "        1.60038982e-04, 3.92413776e-04, 1.79747914e-04, 3.43703165e-04,\n",
       "        4.14284164e-04, 1.30005061e-03, 8.28818846e-04, 2.50092094e-04,\n",
       "        7.92124192e-04, 1.65627175e-04, 4.25604851e-04, 9.09803336e-04]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500}],\n",
       " 'split0_test_score': array([0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.83495146, 0.83809524, 0.84      , 0.8627451 ,\n",
       "        0.86538462, 0.84313725, 0.87378641, 0.86538462, 0.82352941,\n",
       "        0.85436893, 0.83809524, 0.83168317, 0.84615385, 0.85436893,\n",
       "        0.82352941, 0.85436893, 0.85436893, 0.82352941, 0.83495146,\n",
       "        0.83495146, 0.83168317, 0.84313725, 0.84313725, 0.82352941,\n",
       "        0.84615385, 0.86538462, 0.84615385, 0.83495146, 0.82692308,\n",
       "        0.85436893, 0.84313725, 0.84313725, 0.83495146, 0.85714286,\n",
       "        0.83809524, 0.84      , 0.84313725, 0.81553398, 0.8627451 ,\n",
       "        0.85148515, 0.8627451 , 0.85436893, 0.85714286, 0.85436893,\n",
       "        0.80392157, 0.82352941, 0.83809524, 0.85148515, 0.85148515,\n",
       "        0.8627451 , 0.84313725, 0.84313725, 0.86538462, 0.82352941,\n",
       "        0.83495146, 0.85436893, 0.84      , 0.85436893, 0.84615385,\n",
       "        0.82352941, 0.84313725, 0.84615385, 0.80769231, 0.84313725,\n",
       "        0.85148515, 0.82352941, 0.85436893, 0.84615385, 0.81553398,\n",
       "        0.83495146, 0.83809524, 0.80392157, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.84313725, 0.83495146, 0.81553398, 0.85436893,\n",
       "        0.85714286, 0.83168317, 0.84313725, 0.84313725, 0.8627451 ,\n",
       "        0.85148515, 0.84      , 0.82352941, 0.86538462, 0.84615385,\n",
       "        0.82      , 0.82352941, 0.83495146, 0.84536082, 0.84313725,\n",
       "        0.85148515, 0.81632653, 0.84615385, 0.85436893, 0.8       ,\n",
       "        0.83168317, 0.85436893, 0.80808081, 0.84313725, 0.85436893,\n",
       "        0.76767677, 0.85148515, 0.85148515, 0.8       , 0.83168317,\n",
       "        0.8627451 , 0.81632653, 0.84313725, 0.8627451 , 0.7755102 ,\n",
       "        0.82352941, 0.8627451 , 0.82828283, 0.83168317, 0.84313725,\n",
       "        0.82474227, 0.85148515, 0.85436893, 0.79591837, 0.84      ,\n",
       "        0.8627451 , 0.81632653, 0.85148515, 0.85148515, 0.82474227,\n",
       "        0.85148515, 0.8627451 , 0.79591837, 0.84313725, 0.86538462,\n",
       "        0.75555556, 0.8125    , 0.81188119, 0.76404494, 0.82105263,\n",
       "        0.84      , 0.74157303, 0.81632653, 0.84615385, 0.76595745,\n",
       "        0.79591837, 0.84      , 0.77777778, 0.80412371, 0.84848485,\n",
       "        0.71111111, 0.7628866 , 0.82352941, 0.76595745, 0.82      ,\n",
       "        0.83168317, 0.80434783, 0.81632653, 0.84      , 0.75268817,\n",
       "        0.79591837, 0.83168317, 0.77894737, 0.80808081, 0.82      ,\n",
       "        0.78723404, 0.81632653, 0.84      , 0.76086957, 0.78350515,\n",
       "        0.8       , 0.75      , 0.82828283, 0.82      , 0.7826087 ,\n",
       "        0.82474227, 0.82828283, 0.76923077, 0.79591837, 0.83168317]),\n",
       " 'split1_test_score': array([0.74509804, 0.72      , 0.7       , 0.74      , 0.74      ,\n",
       "        0.74509804, 0.74226804, 0.74      , 0.73267327, 0.70833333,\n",
       "        0.71428571, 0.73786408, 0.69387755, 0.7       , 0.7184466 ,\n",
       "        0.72916667, 0.72164948, 0.74226804, 0.68041237, 0.7       ,\n",
       "        0.7184466 , 0.66666667, 0.71428571, 0.75      , 0.72164948,\n",
       "        0.72727273, 0.74747475, 0.69387755, 0.7184466 , 0.71153846,\n",
       "        0.70103093, 0.70707071, 0.74285714, 0.72164948, 0.72      ,\n",
       "        0.76470588, 0.67368421, 0.7184466 , 0.71153846, 0.71287129,\n",
       "        0.72      , 0.74      , 0.70103093, 0.74747475, 0.76767677,\n",
       "        0.7       , 0.72      , 0.73267327, 0.71428571, 0.71428571,\n",
       "        0.74      , 0.72727273, 0.74      , 0.72727273, 0.70103093,\n",
       "        0.71428571, 0.72      , 0.71428571, 0.70103093, 0.71428571,\n",
       "        0.71428571, 0.71428571, 0.70707071, 0.68686869, 0.71428571,\n",
       "        0.71287129, 0.68041237, 0.70103093, 0.68686869, 0.71428571,\n",
       "        0.70707071, 0.71428571, 0.70103093, 0.72      , 0.7254902 ,\n",
       "        0.65979381, 0.71287129, 0.70588235, 0.70103093, 0.70707071,\n",
       "        0.74      , 0.6875    , 0.72727273, 0.7254902 , 0.68041237,\n",
       "        0.73267327, 0.7184466 , 0.70103093, 0.72      , 0.75247525,\n",
       "        0.71428571, 0.72727273, 0.74      , 0.67368421, 0.72      ,\n",
       "        0.71428571, 0.70833333, 0.72727273, 0.71428571, 0.6875    ,\n",
       "        0.70103093, 0.72727273, 0.6875    , 0.71428571, 0.72164948,\n",
       "        0.70833333, 0.71428571, 0.71428571, 0.70103093, 0.68041237,\n",
       "        0.70103093, 0.68686869, 0.70707071, 0.68686869, 0.70707071,\n",
       "        0.71428571, 0.69387755, 0.6875    , 0.69387755, 0.72      ,\n",
       "        0.65979381, 0.68041237, 0.70707071, 0.71428571, 0.70707071,\n",
       "        0.69387755, 0.70103093, 0.70103093, 0.74      , 0.67346939,\n",
       "        0.69387755, 0.71428571, 0.71428571, 0.70707071, 0.70103093,\n",
       "        0.68085106, 0.67346939, 0.7       , 0.66666667, 0.66666667,\n",
       "        0.70707071, 0.66666667, 0.68686869, 0.69387755, 0.66666667,\n",
       "        0.6875    , 0.68686869, 0.65979381, 0.66666667, 0.70707071,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.67346939, 0.68686869,\n",
       "        0.68686869, 0.67346939, 0.69387755, 0.70707071, 0.69387755,\n",
       "        0.68      , 0.70707071, 0.6875    , 0.7       , 0.7       ,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.69387755, 0.68041237, 0.68686869, 0.68686869, 0.66666667,\n",
       "        0.68      , 0.70707071, 0.70707071, 0.68      , 0.7       ,\n",
       "        0.74509804, 0.72      , 0.7       , 0.74      , 0.74      ,\n",
       "        0.74509804, 0.74226804, 0.74      , 0.73267327, 0.70833333,\n",
       "        0.71428571, 0.73786408, 0.69387755, 0.7       , 0.7184466 ,\n",
       "        0.72916667, 0.72164948, 0.74226804, 0.68041237, 0.7       ,\n",
       "        0.7184466 , 0.66666667, 0.71428571, 0.75      , 0.72164948,\n",
       "        0.72727273, 0.74747475, 0.69387755, 0.7184466 , 0.71153846,\n",
       "        0.70103093, 0.70707071, 0.74285714, 0.72164948, 0.72      ,\n",
       "        0.76470588, 0.67368421, 0.7184466 , 0.71153846, 0.71287129,\n",
       "        0.72      , 0.74      , 0.70103093, 0.74747475, 0.76767677,\n",
       "        0.7       , 0.72      , 0.73267327, 0.71428571, 0.71428571,\n",
       "        0.74      , 0.72727273, 0.74      , 0.72727273, 0.70103093,\n",
       "        0.71428571, 0.72      , 0.71428571, 0.70103093, 0.71428571,\n",
       "        0.71428571, 0.71428571, 0.70707071, 0.68686869, 0.71428571,\n",
       "        0.71287129, 0.68041237, 0.70103093, 0.68686869, 0.71428571,\n",
       "        0.70707071, 0.71428571, 0.70103093, 0.72      , 0.7254902 ,\n",
       "        0.65979381, 0.71287129, 0.70588235, 0.70103093, 0.70707071,\n",
       "        0.74      , 0.6875    , 0.72727273, 0.7254902 , 0.68041237,\n",
       "        0.73267327, 0.7184466 , 0.70103093, 0.72      , 0.75247525,\n",
       "        0.71428571, 0.72727273, 0.74      , 0.67368421, 0.72      ,\n",
       "        0.71428571, 0.70833333, 0.72727273, 0.71428571, 0.6875    ,\n",
       "        0.70103093, 0.72727273, 0.6875    , 0.71428571, 0.72164948,\n",
       "        0.70833333, 0.71428571, 0.71428571, 0.70103093, 0.68041237,\n",
       "        0.70103093, 0.68686869, 0.70707071, 0.68686869, 0.70707071,\n",
       "        0.71428571, 0.69387755, 0.6875    , 0.69387755, 0.72      ,\n",
       "        0.65979381, 0.68041237, 0.70707071, 0.71428571, 0.70707071,\n",
       "        0.69387755, 0.70103093, 0.70103093, 0.74      , 0.67346939,\n",
       "        0.69387755, 0.71428571, 0.71428571, 0.70707071, 0.70103093,\n",
       "        0.68085106, 0.67346939, 0.7       , 0.66666667, 0.66666667,\n",
       "        0.70707071, 0.66666667, 0.68686869, 0.69387755, 0.66666667,\n",
       "        0.6875    , 0.68686869, 0.65979381, 0.66666667, 0.70707071,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.67346939, 0.68686869,\n",
       "        0.68686869, 0.67346939, 0.69387755, 0.70707071, 0.69387755,\n",
       "        0.68      , 0.70707071, 0.6875    , 0.7       , 0.7       ,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.69387755, 0.68041237, 0.68686869, 0.68686869, 0.66666667,\n",
       "        0.68      , 0.70707071, 0.70707071, 0.68      , 0.7       ,\n",
       "        0.72      , 0.73267327, 0.74      , 0.7       , 0.72727273,\n",
       "        0.72      , 0.70103093, 0.70707071, 0.71428571, 0.69387755,\n",
       "        0.72727273, 0.69306931, 0.7       , 0.72164948, 0.71428571,\n",
       "        0.69387755, 0.68686869, 0.71428571, 0.67326733, 0.7       ,\n",
       "        0.72380952, 0.7       , 0.70833333, 0.70103093, 0.69387755,\n",
       "        0.7       , 0.70707071, 0.69306931, 0.72      , 0.70588235,\n",
       "        0.70707071, 0.72164948, 0.73267327, 0.69387755, 0.70707071,\n",
       "        0.74      , 0.67326733, 0.73786408, 0.71153846, 0.70707071,\n",
       "        0.70707071, 0.7254902 , 0.69387755, 0.69387755, 0.72      ,\n",
       "        0.66666667, 0.72      , 0.74509804, 0.68      , 0.72      ,\n",
       "        0.71428571, 0.69387755, 0.72      , 0.72727273, 0.67346939,\n",
       "        0.71428571, 0.72      , 0.67346939, 0.70707071, 0.71428571,\n",
       "        0.70707071, 0.68686869, 0.69387755, 0.67326733, 0.69387755,\n",
       "        0.68686869, 0.69306931, 0.72      , 0.71428571, 0.69387755,\n",
       "        0.69387755, 0.69387755, 0.67326733, 0.70707071, 0.72      ,\n",
       "        0.68686869, 0.71428571, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.70707071, 0.66      , 0.72      , 0.71153846, 0.7       ,\n",
       "        0.69387755, 0.72      , 0.69387755, 0.7       , 0.70707071,\n",
       "        0.66666667, 0.73267327, 0.72      , 0.65306122, 0.70707071,\n",
       "        0.70707071, 0.68686869, 0.72      , 0.72      , 0.65306122,\n",
       "        0.68686869, 0.71428571, 0.65306122, 0.70707071, 0.70707071,\n",
       "        0.66666667, 0.70707071, 0.70707071, 0.65306122, 0.67346939,\n",
       "        0.70707071, 0.65306122, 0.7       , 0.69387755, 0.67346939,\n",
       "        0.68686869, 0.68686869, 0.66666667, 0.69306931, 0.70707071,\n",
       "        0.66666667, 0.68686869, 0.70707071, 0.68686869, 0.68686869,\n",
       "        0.7       , 0.66666667, 0.7       , 0.71287129, 0.65306122,\n",
       "        0.68686869, 0.7       , 0.68686869, 0.69387755, 0.7       ,\n",
       "        0.60215054, 0.66666667, 0.66666667, 0.60215054, 0.66666667,\n",
       "        0.68686869, 0.60869565, 0.68      , 0.68686869, 0.63157895,\n",
       "        0.65306122, 0.66666667, 0.61052632, 0.65306122, 0.68      ,\n",
       "        0.63157895, 0.66666667, 0.70707071, 0.625     , 0.65306122,\n",
       "        0.68      , 0.63917526, 0.65306122, 0.69306931, 0.65979381,\n",
       "        0.68041237, 0.70707071, 0.63917526, 0.66      , 0.68627451,\n",
       "        0.63917526, 0.66666667, 0.68686869, 0.65979381, 0.69387755,\n",
       "        0.70707071, 0.63917526, 0.66      , 0.68627451, 0.63157895,\n",
       "        0.67346939, 0.68686869, 0.64583333, 0.69387755, 0.69387755]),\n",
       " 'split2_test_score': array([0.74509804, 0.74285714, 0.73584906, 0.75728155, 0.75      ,\n",
       "        0.75      , 0.75728155, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74766355, 0.73584906, 0.75      , 0.74285714, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.75471698, 0.73076923, 0.72380952,\n",
       "        0.73584906, 0.74285714, 0.74285714, 0.75      , 0.73786408,\n",
       "        0.75728155, 0.74285714, 0.73076923, 0.73584906, 0.73584906,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75      , 0.74285714,\n",
       "        0.75471698, 0.73584906, 0.72222222, 0.72222222, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.73076923, 0.74285714, 0.73394495,\n",
       "        0.76470588, 0.72380952, 0.75471698, 0.76470588, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.73584906, 0.74285714, 0.73786408,\n",
       "        0.72380952, 0.72380952, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.76190476, 0.75      , 0.75      , 0.71153846, 0.72897196,\n",
       "        0.73584906, 0.74285714, 0.76190476, 0.74285714, 0.74285714,\n",
       "        0.75      , 0.75      , 0.71698113, 0.72222222, 0.72897196,\n",
       "        0.73076923, 0.73584906, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.71698113, 0.72222222, 0.72222222, 0.73076923,\n",
       "        0.73584906, 0.75      , 0.74285714, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75728155, 0.75925926, 0.75247525, 0.75728155,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.72380952, 0.75471698, 0.74509804, 0.75      , 0.73584906,\n",
       "        0.73076923, 0.75      , 0.73786408, 0.73076923, 0.74766355,\n",
       "        0.72897196, 0.73076923, 0.74285714, 0.75      , 0.72380952,\n",
       "        0.75      , 0.73786408, 0.72380952, 0.71028037, 0.74074074,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.71698113, 0.71028037, 0.72222222, 0.72380952,\n",
       "        0.73076923, 0.72897196, 0.72380952, 0.73786408, 0.73584906,\n",
       "        0.72727273, 0.74      , 0.75247525, 0.72727273, 0.74      ,\n",
       "        0.75247525, 0.72727273, 0.75247525, 0.75728155, 0.73267327,\n",
       "        0.74509804, 0.75247525, 0.74      , 0.75247525, 0.74509804,\n",
       "        0.74      , 0.75728155, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.73076923, 0.75728155, 0.73786408, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73267327, 0.73076923, 0.71698113,\n",
       "        0.75247525, 0.73076923, 0.73076923, 0.72380952, 0.74285714,\n",
       "        0.74285714, 0.73267327, 0.71698113, 0.71698113, 0.75247525,\n",
       "        0.72380952, 0.73076923, 0.72380952, 0.74285714, 0.74285714,\n",
       "        0.74509804, 0.74285714, 0.73584906, 0.75728155, 0.75      ,\n",
       "        0.75      , 0.75728155, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74766355, 0.73584906, 0.75      , 0.74285714, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.75471698, 0.73076923, 0.72380952,\n",
       "        0.73584906, 0.74285714, 0.74285714, 0.75      , 0.73786408,\n",
       "        0.75728155, 0.74285714, 0.73076923, 0.73584906, 0.73584906,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75      , 0.74285714,\n",
       "        0.75471698, 0.73584906, 0.72222222, 0.72222222, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.73076923, 0.74285714, 0.73394495,\n",
       "        0.76470588, 0.72380952, 0.75471698, 0.76470588, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.73584906, 0.74285714, 0.73786408,\n",
       "        0.72380952, 0.72380952, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.76190476, 0.75      , 0.75      , 0.71153846, 0.72897196,\n",
       "        0.73584906, 0.74285714, 0.76190476, 0.74285714, 0.74285714,\n",
       "        0.75      , 0.75      , 0.71698113, 0.72222222, 0.72897196,\n",
       "        0.73076923, 0.73584906, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.71698113, 0.72222222, 0.72222222, 0.73076923,\n",
       "        0.73584906, 0.75      , 0.74285714, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75728155, 0.75925926, 0.75247525, 0.75728155,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.72380952, 0.75471698, 0.74509804, 0.75      , 0.73584906,\n",
       "        0.73076923, 0.75      , 0.73786408, 0.73076923, 0.74766355,\n",
       "        0.72897196, 0.73076923, 0.74285714, 0.75      , 0.72380952,\n",
       "        0.75      , 0.73786408, 0.72380952, 0.71028037, 0.74074074,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.71698113, 0.71028037, 0.72222222, 0.72380952,\n",
       "        0.73076923, 0.72897196, 0.72380952, 0.73786408, 0.73584906,\n",
       "        0.72727273, 0.74      , 0.75247525, 0.72727273, 0.74      ,\n",
       "        0.75247525, 0.72727273, 0.75247525, 0.75728155, 0.73267327,\n",
       "        0.74509804, 0.75247525, 0.74      , 0.75247525, 0.74509804,\n",
       "        0.74      , 0.75728155, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.73076923, 0.75728155, 0.73786408, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73267327, 0.73076923, 0.71698113,\n",
       "        0.75247525, 0.73076923, 0.73076923, 0.72380952, 0.74285714,\n",
       "        0.74285714, 0.73267327, 0.71698113, 0.71698113, 0.75247525,\n",
       "        0.72380952, 0.73076923, 0.72380952, 0.74285714, 0.74285714,\n",
       "        0.75247525, 0.75471698, 0.72897196, 0.73076923, 0.74074074,\n",
       "        0.75      , 0.75      , 0.73584906, 0.75      , 0.72380952,\n",
       "        0.73786408, 0.74074074, 0.73584906, 0.75471698, 0.74766355,\n",
       "        0.75      , 0.75      , 0.75471698, 0.73786408, 0.73394495,\n",
       "        0.73394495, 0.73584906, 0.76190476, 0.73584906, 0.73786408,\n",
       "        0.74285714, 0.74285714, 0.72380952, 0.73394495, 0.72222222,\n",
       "        0.73076923, 0.72897196, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.75471698, 0.71153846, 0.72897196, 0.72222222, 0.74285714,\n",
       "        0.74074074, 0.73584906, 0.72380952, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75925926, 0.75229358, 0.73267327, 0.74766355,\n",
       "        0.73584906, 0.74509804, 0.75      , 0.74285714, 0.75247525,\n",
       "        0.7254902 , 0.73584906, 0.73267327, 0.74285714, 0.75471698,\n",
       "        0.74509804, 0.75      , 0.76190476, 0.73267327, 0.74285714,\n",
       "        0.72897196, 0.7254902 , 0.75471698, 0.74766355, 0.73786408,\n",
       "        0.74285714, 0.74285714, 0.7184466 , 0.74545455, 0.73394495,\n",
       "        0.71698113, 0.75471698, 0.74285714, 0.73786408, 0.73584906,\n",
       "        0.74285714, 0.7047619 , 0.73873874, 0.73394495, 0.73076923,\n",
       "        0.74766355, 0.73584906, 0.73076923, 0.73584906, 0.74285714,\n",
       "        0.74      , 0.76923077, 0.74766355, 0.74      , 0.75471698,\n",
       "        0.75471698, 0.74      , 0.76190476, 0.75471698, 0.74509804,\n",
       "        0.75      , 0.76635514, 0.74509804, 0.75      , 0.74285714,\n",
       "        0.73267327, 0.75      , 0.76190476, 0.74509804, 0.73584906,\n",
       "        0.75925926, 0.74509804, 0.74285714, 0.75471698, 0.73786408,\n",
       "        0.74285714, 0.76190476, 0.73267327, 0.74285714, 0.74545455,\n",
       "        0.74509804, 0.73786408, 0.75471698, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.74509804, 0.75471698, 0.74545455, 0.73786408,\n",
       "        0.74285714, 0.74766355, 0.73076923, 0.74285714, 0.74285714,\n",
       "        0.70833333, 0.74      , 0.75247525, 0.70833333, 0.73267327,\n",
       "        0.74509804, 0.75510204, 0.74      , 0.76470588, 0.74      ,\n",
       "        0.75728155, 0.75728155, 0.74747475, 0.74509804, 0.74509804,\n",
       "        0.74747475, 0.73786408, 0.75      , 0.76      , 0.75      ,\n",
       "        0.75      , 0.75247525, 0.74509804, 0.73076923, 0.74      ,\n",
       "        0.73786408, 0.74285714, 0.76      , 0.75      , 0.72222222,\n",
       "        0.74      , 0.73786408, 0.73076923, 0.74509804, 0.73786408,\n",
       "        0.75      , 0.74      , 0.74285714, 0.72897196, 0.74      ,\n",
       "        0.73786408, 0.72380952, 0.74509804, 0.73076923, 0.75      ]),\n",
       " 'split3_test_score': array([0.76470588, 0.78095238, 0.78846154, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.78846154, 0.80769231, 0.80769231, 0.8       ,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.77669903, 0.80769231, 0.81904762, 0.80769231, 0.78095238,\n",
       "        0.77358491, 0.81553398, 0.78846154, 0.77669903, 0.7961165 ,\n",
       "        0.81553398, 0.80392157, 0.80769231, 0.78846154, 0.78846154,\n",
       "        0.80769231, 0.78846154, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.81553398, 0.78846154, 0.78095238, 0.77669903, 0.8       ,\n",
       "        0.76923077, 0.77358491, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.76470588, 0.78846154, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.77227723, 0.79245283, 0.8       , 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.77669903, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.78095238, 0.78095238,\n",
       "        0.77777778, 0.7961165 , 0.78846154, 0.78095238, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.81553398, 0.78095238, 0.76923077,\n",
       "        0.80769231, 0.8       , 0.78095238, 0.80769231, 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76923077, 0.80769231,\n",
       "        0.79245283, 0.8       , 0.80769231, 0.80769231, 0.7961165 ,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.76470588, 0.78846154,\n",
       "        0.8       , 0.77227723, 0.78846154, 0.80769231, 0.78431373,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.79245283, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.80392157, 0.7961165 ,\n",
       "        0.7961165 , 0.78095238, 0.8       , 0.79245283, 0.80392157,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.80392157, 0.78095238,\n",
       "        0.80769231, 0.8       , 0.79245283, 0.80392157, 0.80769231,\n",
       "        0.80769231, 0.80769231, 0.78846154, 0.78095238, 0.80769231,\n",
       "        0.79245283, 0.78504673, 0.80392157, 0.80769231, 0.80769231,\n",
       "        0.75510204, 0.76470588, 0.77669903, 0.75510204, 0.76470588,\n",
       "        0.76923077, 0.74      , 0.76470588, 0.76923077, 0.77227723,\n",
       "        0.76470588, 0.78846154, 0.76470588, 0.77669903, 0.76923077,\n",
       "        0.75728155, 0.77669903, 0.76923077, 0.80392157, 0.7961165 ,\n",
       "        0.80392157, 0.80392157, 0.7961165 , 0.7961165 , 0.77669903,\n",
       "        0.78431373, 0.7961165 , 0.80392157, 0.78431373, 0.81553398,\n",
       "        0.79207921, 0.7961165 , 0.80769231, 0.77669903, 0.7961165 ,\n",
       "        0.80769231, 0.80392157, 0.80392157, 0.81553398, 0.79207921,\n",
       "        0.80769231, 0.8       , 0.77669903, 0.7961165 , 0.81553398,\n",
       "        0.76470588, 0.78095238, 0.78846154, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.78846154, 0.80769231, 0.80769231, 0.8       ,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.77669903, 0.80769231, 0.81904762, 0.80769231, 0.78095238,\n",
       "        0.77358491, 0.81553398, 0.78846154, 0.77669903, 0.7961165 ,\n",
       "        0.81553398, 0.80392157, 0.80769231, 0.78846154, 0.78846154,\n",
       "        0.80769231, 0.78846154, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.81553398, 0.78846154, 0.78095238, 0.77669903, 0.8       ,\n",
       "        0.76923077, 0.77358491, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.76470588, 0.78846154, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.77227723, 0.79245283, 0.8       , 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.77669903, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.78095238, 0.78095238,\n",
       "        0.77777778, 0.7961165 , 0.78846154, 0.78095238, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.81553398, 0.78095238, 0.76923077,\n",
       "        0.80769231, 0.8       , 0.78095238, 0.80769231, 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76923077, 0.80769231,\n",
       "        0.79245283, 0.8       , 0.80769231, 0.80769231, 0.7961165 ,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.76470588, 0.78846154,\n",
       "        0.8       , 0.77227723, 0.78846154, 0.80769231, 0.78431373,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.79245283, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.80392157, 0.7961165 ,\n",
       "        0.7961165 , 0.78095238, 0.8       , 0.79245283, 0.80392157,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.80392157, 0.78095238,\n",
       "        0.80769231, 0.8       , 0.79245283, 0.80392157, 0.80769231,\n",
       "        0.80769231, 0.80769231, 0.78846154, 0.78095238, 0.80769231,\n",
       "        0.79245283, 0.78504673, 0.80392157, 0.80769231, 0.80769231,\n",
       "        0.75510204, 0.76470588, 0.77669903, 0.75510204, 0.76470588,\n",
       "        0.76923077, 0.74      , 0.76470588, 0.76923077, 0.77227723,\n",
       "        0.76470588, 0.78846154, 0.76470588, 0.77669903, 0.76923077,\n",
       "        0.75728155, 0.77669903, 0.76923077, 0.80392157, 0.7961165 ,\n",
       "        0.80392157, 0.80392157, 0.7961165 , 0.7961165 , 0.77669903,\n",
       "        0.78431373, 0.7961165 , 0.80392157, 0.78431373, 0.81553398,\n",
       "        0.79207921, 0.7961165 , 0.80769231, 0.77669903, 0.7961165 ,\n",
       "        0.80769231, 0.80392157, 0.80392157, 0.81553398, 0.79207921,\n",
       "        0.80769231, 0.8       , 0.77669903, 0.7961165 , 0.81553398,\n",
       "        0.77669903, 0.79245283, 0.79245283, 0.77669903, 0.78095238,\n",
       "        0.78846154, 0.77669903, 0.81132075, 0.7961165 , 0.80769231,\n",
       "        0.8       , 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.7961165 , 0.81904762, 0.82692308, 0.82692308, 0.79245283,\n",
       "        0.79245283, 0.82692308, 0.79245283, 0.78504673, 0.80769231,\n",
       "        0.81904762, 0.81904762, 0.81553398, 0.78846154, 0.77358491,\n",
       "        0.82692308, 0.77358491, 0.78504673, 0.81553398, 0.80769231,\n",
       "        0.81132075, 0.80769231, 0.79245283, 0.77777778, 0.80769231,\n",
       "        0.78504673, 0.78504673, 0.82692308, 0.81132075, 0.8       ,\n",
       "        0.77669903, 0.81132075, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.78431373, 0.8       , 0.7961165 , 0.7961165 ,\n",
       "        0.8       , 0.78095238, 0.78431373, 0.8       , 0.78846154,\n",
       "        0.78431373, 0.80769231, 0.80769231, 0.80392157, 0.78095238,\n",
       "        0.78504673, 0.7961165 , 0.79245283, 0.78504673, 0.78846154,\n",
       "        0.8       , 0.81904762, 0.82692308, 0.78846154, 0.77358491,\n",
       "        0.80769231, 0.78846154, 0.78504673, 0.80769231, 0.80769231,\n",
       "        0.8       , 0.80769231, 0.78095238, 0.78095238, 0.80769231,\n",
       "        0.78504673, 0.79245283, 0.81553398, 0.81904762, 0.81132075,\n",
       "        0.78      , 0.76923077, 0.78095238, 0.76470588, 0.78095238,\n",
       "        0.79245283, 0.76470588, 0.78095238, 0.78095238, 0.77669903,\n",
       "        0.80373832, 0.78095238, 0.77669903, 0.79245283, 0.79245283,\n",
       "        0.77669903, 0.79245283, 0.8       , 0.7961165 , 0.79245283,\n",
       "        0.79245283, 0.7961165 , 0.79245283, 0.79245283, 0.77669903,\n",
       "        0.79245283, 0.8       , 0.7961165 , 0.78846154, 0.78095238,\n",
       "        0.77669903, 0.8       , 0.79245283, 0.7961165 , 0.8       ,\n",
       "        0.81132075, 0.78846154, 0.78095238, 0.79245283, 0.78846154,\n",
       "        0.79245283, 0.7962963 , 0.7961165 , 0.81132075, 0.8       ,\n",
       "        0.77419355, 0.78787879, 0.76470588, 0.77419355, 0.78787879,\n",
       "        0.76470588, 0.75789474, 0.78787879, 0.76470588, 0.77083333,\n",
       "        0.78      , 0.77669903, 0.7628866 , 0.78      , 0.77669903,\n",
       "        0.7755102 , 0.78      , 0.77669903, 0.82828283, 0.76470588,\n",
       "        0.78846154, 0.78787879, 0.76470588, 0.78846154, 0.7755102 ,\n",
       "        0.76      , 0.78846154, 0.79166667, 0.77227723, 0.8       ,\n",
       "        0.7628866 , 0.76470588, 0.79245283, 0.7628866 , 0.76      ,\n",
       "        0.8       , 0.8125    , 0.77669903, 0.78846154, 0.7628866 ,\n",
       "        0.76923077, 0.8       , 0.7628866 , 0.78      , 0.80769231]),\n",
       " 'split4_test_score': array([0.73684211, 0.8       , 0.81188119, 0.74747475, 0.76      ,\n",
       "        0.75      , 0.72164948, 0.73469388, 0.75510204, 0.75510204,\n",
       "        0.8       , 0.77083333, 0.74      , 0.7628866 , 0.7628866 ,\n",
       "        0.71428571, 0.74468085, 0.75      , 0.81632653, 0.83673469,\n",
       "        0.82828283, 0.79591837, 0.77083333, 0.78350515, 0.74226804,\n",
       "        0.76595745, 0.75      , 0.79591837, 0.81632653, 0.81632653,\n",
       "        0.78787879, 0.81632653, 0.78787879, 0.76      , 0.75789474,\n",
       "        0.7755102 , 0.79591837, 0.82828283, 0.79591837, 0.78      ,\n",
       "        0.78787879, 0.7755102 , 0.78      , 0.75789474, 0.7628866 ,\n",
       "        0.73684211, 0.8       , 0.79591837, 0.72916667, 0.76767677,\n",
       "        0.77083333, 0.72916667, 0.70833333, 0.73469388, 0.75510204,\n",
       "        0.78350515, 0.8       , 0.72727273, 0.7628866 , 0.75      ,\n",
       "        0.74      , 0.72164948, 0.76595745, 0.7755102 , 0.80412371,\n",
       "        0.80412371, 0.74226804, 0.7628866 , 0.75      , 0.75510204,\n",
       "        0.74226804, 0.75789474, 0.79591837, 0.82      , 0.81632653,\n",
       "        0.78350515, 0.7628866 , 0.78350515, 0.7755102 , 0.75789474,\n",
       "        0.75789474, 0.79207921, 0.82      , 0.82828283, 0.79591837,\n",
       "        0.78      , 0.78787879, 0.76767677, 0.75789474, 0.75789474,\n",
       "        0.73684211, 0.75510204, 0.8       , 0.72340426, 0.73267327,\n",
       "        0.75510204, 0.71578947, 0.72727273, 0.70833333, 0.74226804,\n",
       "        0.78787879, 0.8       , 0.73469388, 0.77227723, 0.78      ,\n",
       "        0.71428571, 0.70707071, 0.71428571, 0.77083333, 0.78787879,\n",
       "        0.8125    , 0.75      , 0.76470588, 0.7755102 , 0.75      ,\n",
       "        0.72164948, 0.75789474, 0.80412371, 0.80808081, 0.80808081,\n",
       "        0.77083333, 0.81188119, 0.8       , 0.75      , 0.7628866 ,\n",
       "        0.74468085, 0.80808081, 0.80392157, 0.82828283, 0.78350515,\n",
       "        0.81188119, 0.8       , 0.75789474, 0.7628866 , 0.74468085,\n",
       "        0.73333333, 0.73684211, 0.73684211, 0.73333333, 0.73684211,\n",
       "        0.73684211, 0.73333333, 0.72340426, 0.72916667, 0.74725275,\n",
       "        0.77083333, 0.75789474, 0.73333333, 0.73684211, 0.75      ,\n",
       "        0.73333333, 0.7628866 , 0.75510204, 0.76595745, 0.77083333,\n",
       "        0.77083333, 0.7311828 , 0.7628866 , 0.7628866 , 0.7173913 ,\n",
       "        0.75      , 0.76767677, 0.77419355, 0.78350515, 0.8       ,\n",
       "        0.76595745, 0.75789474, 0.76767677, 0.70967742, 0.74226804,\n",
       "        0.7628866 , 0.79569892, 0.76767677, 0.79207921, 0.76595745,\n",
       "        0.7755102 , 0.8       , 0.7173913 , 0.7628866 , 0.7755102 ,\n",
       "        0.73684211, 0.8       , 0.81188119, 0.74747475, 0.76      ,\n",
       "        0.75      , 0.72164948, 0.73469388, 0.75510204, 0.75510204,\n",
       "        0.8       , 0.77083333, 0.74      , 0.7628866 , 0.7628866 ,\n",
       "        0.71428571, 0.74468085, 0.75      , 0.81632653, 0.83673469,\n",
       "        0.82828283, 0.79591837, 0.77083333, 0.78350515, 0.74226804,\n",
       "        0.76595745, 0.75      , 0.79591837, 0.81632653, 0.81632653,\n",
       "        0.78787879, 0.81632653, 0.78787879, 0.76      , 0.75789474,\n",
       "        0.7755102 , 0.79591837, 0.82828283, 0.79591837, 0.78      ,\n",
       "        0.78787879, 0.7755102 , 0.78      , 0.75789474, 0.7628866 ,\n",
       "        0.73684211, 0.8       , 0.79591837, 0.72916667, 0.76767677,\n",
       "        0.77083333, 0.72916667, 0.70833333, 0.73469388, 0.75510204,\n",
       "        0.78350515, 0.8       , 0.72727273, 0.7628866 , 0.75      ,\n",
       "        0.74      , 0.72164948, 0.76595745, 0.7755102 , 0.80412371,\n",
       "        0.80412371, 0.74226804, 0.7628866 , 0.75      , 0.75510204,\n",
       "        0.74226804, 0.75789474, 0.79591837, 0.82      , 0.81632653,\n",
       "        0.78350515, 0.7628866 , 0.78350515, 0.7755102 , 0.75789474,\n",
       "        0.75789474, 0.79207921, 0.82      , 0.82828283, 0.79591837,\n",
       "        0.78      , 0.78787879, 0.76767677, 0.75789474, 0.75789474,\n",
       "        0.73684211, 0.75510204, 0.8       , 0.72340426, 0.73267327,\n",
       "        0.75510204, 0.71578947, 0.72727273, 0.70833333, 0.74226804,\n",
       "        0.78787879, 0.8       , 0.73469388, 0.77227723, 0.78      ,\n",
       "        0.71428571, 0.70707071, 0.71428571, 0.77083333, 0.78787879,\n",
       "        0.8125    , 0.75      , 0.76470588, 0.7755102 , 0.75      ,\n",
       "        0.72164948, 0.75789474, 0.80412371, 0.80808081, 0.80808081,\n",
       "        0.77083333, 0.81188119, 0.8       , 0.75      , 0.7628866 ,\n",
       "        0.74468085, 0.80808081, 0.80392157, 0.82828283, 0.78350515,\n",
       "        0.81188119, 0.8       , 0.75789474, 0.7628866 , 0.74468085,\n",
       "        0.73333333, 0.73684211, 0.73684211, 0.73333333, 0.73684211,\n",
       "        0.73684211, 0.73333333, 0.72340426, 0.72916667, 0.74725275,\n",
       "        0.77083333, 0.75789474, 0.73333333, 0.73684211, 0.75      ,\n",
       "        0.73333333, 0.7628866 , 0.75510204, 0.76595745, 0.77083333,\n",
       "        0.77083333, 0.7311828 , 0.7628866 , 0.7628866 , 0.7173913 ,\n",
       "        0.75      , 0.76767677, 0.77419355, 0.78350515, 0.8       ,\n",
       "        0.76595745, 0.75789474, 0.76767677, 0.70967742, 0.74226804,\n",
       "        0.7628866 , 0.79569892, 0.76767677, 0.79207921, 0.76595745,\n",
       "        0.7755102 , 0.8       , 0.7173913 , 0.7628866 , 0.7755102 ,\n",
       "        0.74226804, 0.79207921, 0.80392157, 0.72727273, 0.76      ,\n",
       "        0.76      , 0.73469388, 0.72      , 0.72727273, 0.74226804,\n",
       "        0.79207921, 0.79166667, 0.72164948, 0.76      , 0.75510204,\n",
       "        0.74747475, 0.73267327, 0.74226804, 0.76470588, 0.79591837,\n",
       "        0.80412371, 0.72727273, 0.75247525, 0.76767677, 0.73469388,\n",
       "        0.75510204, 0.7755102 , 0.78431373, 0.78787879, 0.78350515,\n",
       "        0.79207921, 0.76767677, 0.76767677, 0.76767677, 0.76      ,\n",
       "        0.7755102 , 0.79207921, 0.79591837, 0.78350515, 0.79207921,\n",
       "        0.76767677, 0.77227723, 0.7755102 , 0.76767677, 0.7755102 ,\n",
       "        0.73684211, 0.78431373, 0.8       , 0.74226804, 0.77669903,\n",
       "        0.76470588, 0.73684211, 0.72      , 0.72727273, 0.72164948,\n",
       "        0.78      , 0.8125    , 0.72727273, 0.76470588, 0.74747475,\n",
       "        0.73469388, 0.7254902 , 0.72727273, 0.73469388, 0.78431373,\n",
       "        0.82474227, 0.72727273, 0.74509804, 0.75247525, 0.74226804,\n",
       "        0.74509804, 0.73469388, 0.76767677, 0.8       , 0.79591837,\n",
       "        0.74747475, 0.75247525, 0.7755102 , 0.7628866 , 0.74      ,\n",
       "        0.73469388, 0.76      , 0.78      , 0.80808081, 0.78      ,\n",
       "        0.75247525, 0.76767677, 0.74226804, 0.74      , 0.73469388,\n",
       "        0.74157303, 0.75510204, 0.77227723, 0.72527473, 0.74      ,\n",
       "        0.7961165 , 0.72340426, 0.73469388, 0.73267327, 0.73913043,\n",
       "        0.75247525, 0.77227723, 0.72340426, 0.73267327, 0.76470588,\n",
       "        0.71578947, 0.71287129, 0.7254902 , 0.73684211, 0.77669903,\n",
       "        0.78787879, 0.72916667, 0.76923077, 0.76470588, 0.72340426,\n",
       "        0.73267327, 0.75247525, 0.72916667, 0.76470588, 0.80808081,\n",
       "        0.73684211, 0.75      , 0.76470588, 0.71578947, 0.73267327,\n",
       "        0.74      , 0.75      , 0.76470588, 0.81188119, 0.75      ,\n",
       "        0.77669903, 0.78      , 0.73684211, 0.73267327, 0.74747475,\n",
       "        0.71264368, 0.73333333, 0.72916667, 0.68235294, 0.73333333,\n",
       "        0.72164948, 0.6744186 , 0.72340426, 0.72164948, 0.69565217,\n",
       "        0.71578947, 0.73469388, 0.70967742, 0.72916667, 0.73469388,\n",
       "        0.65934066, 0.73469388, 0.72727273, 0.7032967 , 0.75      ,\n",
       "        0.73469388, 0.70967742, 0.74226804, 0.72727273, 0.68131868,\n",
       "        0.73469388, 0.74747475, 0.7173913 , 0.74226804, 0.74747475,\n",
       "        0.72527473, 0.73684211, 0.75247525, 0.68131868, 0.73469388,\n",
       "        0.75510204, 0.7173913 , 0.74226804, 0.78431373, 0.7173913 ,\n",
       "        0.73684211, 0.76470588, 0.68888889, 0.70833333, 0.76      ]),\n",
       " 'mean_test_score': array([0.76400538, 0.77963569, 0.7748574 , 0.77634054, 0.78244959,\n",
       "        0.77978884, 0.77448114, 0.77812559, 0.78362676, 0.76759514,\n",
       "        0.77945412, 0.77433054, 0.76540296, 0.77139008, 0.76954606,\n",
       "        0.7665793 , 0.78082263, 0.78408031, 0.77904009, 0.78084834,\n",
       "        0.7843096 , 0.77449226, 0.771915  , 0.78291462, 0.77212864,\n",
       "        0.78796642, 0.78310812, 0.77820051, 0.78211378, 0.77742541,\n",
       "        0.78186942, 0.78181697, 0.78178288, 0.78041738, 0.77424746,\n",
       "        0.79072086, 0.77133165, 0.77860826, 0.76508514, 0.77801947,\n",
       "        0.77486713, 0.77586262, 0.77477228, 0.77974241, 0.780891  ,\n",
       "        0.75725077, 0.77675124, 0.78372599, 0.7643184 , 0.77460189,\n",
       "        0.7829359 , 0.76838147, 0.76620083, 0.77404167, 0.75886795,\n",
       "        0.77508931, 0.77655581, 0.75748404, 0.76955188, 0.77134771,\n",
       "        0.76330664, 0.77348278, 0.77757266, 0.76067092, 0.77821577,\n",
       "        0.77867339, 0.76033081, 0.77373055, 0.75524244, 0.76866257,\n",
       "        0.77448313, 0.77347071, 0.77389288, 0.78118394, 0.77830092,\n",
       "        0.76497955, 0.77539831, 0.77351319, 0.76999083, 0.77453155,\n",
       "        0.77880225, 0.77114756, 0.7803865 , 0.77827597, 0.77383224,\n",
       "        0.7774258 , 0.78213886, 0.77084172, 0.77787213, 0.78253772,\n",
       "        0.76083262, 0.77362357, 0.78303262, 0.75255089, 0.7699803 ,\n",
       "        0.774998  , 0.75462695, 0.7709315 , 0.76529304, 0.75678441,\n",
       "        0.76888048, 0.78513744, 0.75445475, 0.77443061, 0.78004873,\n",
       "        0.75401746, 0.76289874, 0.76958284, 0.76865795, 0.77441424,\n",
       "        0.77859767, 0.757065  , 0.77492675, 0.77351536, 0.76496036,\n",
       "        0.77127452, 0.77254266, 0.77185001, 0.77578108, 0.78250381,\n",
       "        0.76181774, 0.78178742, 0.78323342, 0.76545187, 0.77339977,\n",
       "        0.77089849, 0.77309367, 0.77103591, 0.78458852, 0.76569527,\n",
       "        0.78055344, 0.7782099 , 0.76798231, 0.77397652, 0.76872442,\n",
       "        0.74352236, 0.75035041, 0.76290025, 0.74068548, 0.74898987,\n",
       "        0.76282074, 0.73515667, 0.75283775, 0.75556787, 0.74377398,\n",
       "        0.76097439, 0.76683701, 0.74377713, 0.75388355, 0.76397687,\n",
       "        0.74081685, 0.76005371, 0.7642807 , 0.76182812, 0.76426449,\n",
       "        0.76817553, 0.75738159, 0.76784592, 0.76906558, 0.7426132 ,\n",
       "        0.76006162, 0.7721728 , 0.75965768, 0.76706456, 0.77384996,\n",
       "        0.76762959, 0.75594599, 0.7706418 , 0.74815964, 0.76224953,\n",
       "        0.77009017, 0.76920789, 0.7607462 , 0.76794917, 0.76450788,\n",
       "        0.76305897, 0.77556799, 0.75065068, 0.7649995 , 0.77540772,\n",
       "        0.76400538, 0.77963569, 0.7748574 , 0.77634054, 0.78244959,\n",
       "        0.77978884, 0.77448114, 0.77812559, 0.78362676, 0.76759514,\n",
       "        0.77945412, 0.77433054, 0.76540296, 0.77139008, 0.76954606,\n",
       "        0.7665793 , 0.78082263, 0.78408031, 0.77904009, 0.78084834,\n",
       "        0.7843096 , 0.77449226, 0.771915  , 0.78291462, 0.77212864,\n",
       "        0.78796642, 0.78310812, 0.77820051, 0.78211378, 0.77742541,\n",
       "        0.78186942, 0.78181697, 0.78178288, 0.78041738, 0.77424746,\n",
       "        0.79072086, 0.77133165, 0.77860826, 0.76508514, 0.77801947,\n",
       "        0.77486713, 0.77586262, 0.77477228, 0.77974241, 0.780891  ,\n",
       "        0.75725077, 0.77675124, 0.78372599, 0.7643184 , 0.77460189,\n",
       "        0.7829359 , 0.76838147, 0.76620083, 0.77404167, 0.75886795,\n",
       "        0.77508931, 0.77655581, 0.75748404, 0.76955188, 0.77134771,\n",
       "        0.76330664, 0.77348278, 0.77757266, 0.76067092, 0.77821577,\n",
       "        0.77867339, 0.76033081, 0.77373055, 0.75524244, 0.76866257,\n",
       "        0.77448313, 0.77347071, 0.77389288, 0.78118394, 0.77830092,\n",
       "        0.76497955, 0.77539831, 0.77351319, 0.76999083, 0.77453155,\n",
       "        0.77880225, 0.77114756, 0.7803865 , 0.77827597, 0.77383224,\n",
       "        0.7774258 , 0.78213886, 0.77084172, 0.77787213, 0.78253772,\n",
       "        0.76083262, 0.77362357, 0.78303262, 0.75255089, 0.7699803 ,\n",
       "        0.774998  , 0.75462695, 0.7709315 , 0.76529304, 0.75678441,\n",
       "        0.76888048, 0.78513744, 0.75445475, 0.77443061, 0.78004873,\n",
       "        0.75401746, 0.76289874, 0.76958284, 0.76865795, 0.77441424,\n",
       "        0.77859767, 0.757065  , 0.77492675, 0.77351536, 0.76496036,\n",
       "        0.77127452, 0.77254266, 0.77185001, 0.77578108, 0.78250381,\n",
       "        0.76181774, 0.78178742, 0.78323342, 0.76545187, 0.77339977,\n",
       "        0.77089849, 0.77309367, 0.77103591, 0.78458852, 0.76569527,\n",
       "        0.78055344, 0.7782099 , 0.76798231, 0.77397652, 0.76872442,\n",
       "        0.74352236, 0.75035041, 0.76290025, 0.74068548, 0.74898987,\n",
       "        0.76282074, 0.73515667, 0.75283775, 0.75556787, 0.74377398,\n",
       "        0.76097439, 0.76683701, 0.74377713, 0.75388355, 0.76397687,\n",
       "        0.74081685, 0.76005371, 0.7642807 , 0.76182812, 0.76426449,\n",
       "        0.76817553, 0.75738159, 0.76784592, 0.76906558, 0.7426132 ,\n",
       "        0.76006162, 0.7721728 , 0.75965768, 0.76706456, 0.77384996,\n",
       "        0.76762959, 0.75594599, 0.7706418 , 0.74815964, 0.76224953,\n",
       "        0.77009017, 0.76920789, 0.7607462 , 0.76794917, 0.76450788,\n",
       "        0.76305897, 0.77556799, 0.75065068, 0.7649995 , 0.77540772,\n",
       "        0.7646251 , 0.78137475, 0.78068832, 0.7549482 , 0.77434219,\n",
       "        0.77676923, 0.76111222, 0.76960539, 0.77061191, 0.75823537,\n",
       "        0.78231699, 0.76890487, 0.75783634, 0.77419637, 0.77277461,\n",
       "        0.76219964, 0.7685917 , 0.77851255, 0.76525796, 0.77145352,\n",
       "        0.7778565 , 0.76434561, 0.77166069, 0.76654815, 0.75953145,\n",
       "        0.77263213, 0.78197406, 0.77257608, 0.77304735, 0.76242354,\n",
       "        0.78224223, 0.76700408, 0.77427823, 0.7685618 , 0.7749526 ,\n",
       "        0.78392864, 0.76491546, 0.7796689 , 0.76211552, 0.78248889,\n",
       "        0.77040402, 0.77628166, 0.77489786, 0.7731734 , 0.78091922,\n",
       "        0.74732092, 0.77968463, 0.78328785, 0.7566251 , 0.77766011,\n",
       "        0.77320946, 0.76065374, 0.76662745, 0.77178074, 0.75344801,\n",
       "        0.77094547, 0.78073407, 0.75154582, 0.77380053, 0.77021857,\n",
       "        0.75894115, 0.76263769, 0.76738024, 0.75044967, 0.76902761,\n",
       "        0.77542296, 0.75309563, 0.77332736, 0.76912502, 0.75560104,\n",
       "        0.76335684, 0.76571429, 0.75804707, 0.77682481, 0.7733171 ,\n",
       "        0.75814001, 0.77061535, 0.76908725, 0.7635709 , 0.7649558 ,\n",
       "        0.76835292, 0.75282748, 0.77256567, 0.77553077, 0.77624133,\n",
       "        0.76610965, 0.77119573, 0.76119564, 0.77205626, 0.76841927,\n",
       "        0.74964794, 0.76995325, 0.77116892, 0.74568053, 0.76517546,\n",
       "        0.78036843, 0.74626107, 0.76874097, 0.76854231, 0.74279775,\n",
       "        0.76495308, 0.77764788, 0.74126867, 0.76506681, 0.7722911 ,\n",
       "        0.73190104, 0.76277599, 0.76919016, 0.74622357, 0.76203069,\n",
       "        0.78188134, 0.74795379, 0.7695356 , 0.77369967, 0.73738939,\n",
       "        0.75567627, 0.77279876, 0.75058119, 0.76415541, 0.77693914,\n",
       "        0.75000962, 0.76524358, 0.77466307, 0.74509245, 0.76047982,\n",
       "        0.7713846 , 0.75331055, 0.77037208, 0.782829  , 0.75082582,\n",
       "        0.77007257, 0.77734099, 0.74930298, 0.76477319, 0.7711433 ,\n",
       "        0.71057533, 0.74807576, 0.74497913, 0.70621506, 0.74832094,\n",
       "        0.75166442, 0.70753681, 0.74952191, 0.75681676, 0.72080438,\n",
       "        0.74041012, 0.75506823, 0.72166857, 0.74228993, 0.75699516,\n",
       "        0.70500313, 0.73642224, 0.75691438, 0.7365074 , 0.74755342,\n",
       "        0.75696772, 0.73871091, 0.74429194, 0.75591456, 0.72186217,\n",
       "        0.74177774, 0.76350946, 0.73743612, 0.74652522, 0.7551943 ,\n",
       "        0.73091412, 0.74448105, 0.7605132 , 0.72199334, 0.74198813,\n",
       "        0.76243455, 0.73181331, 0.75002141, 0.76160435, 0.72689311,\n",
       "        0.74842972, 0.76073338, 0.72238753, 0.7417797 , 0.76865061]),\n",
       " 'std_test_score': array([0.03342125, 0.04671311, 0.05035899, 0.03967073, 0.04845098,\n",
       "        0.04556326, 0.04921518, 0.05113133, 0.0476812 , 0.04344504,\n",
       "        0.04752719, 0.04006319, 0.05145488, 0.05407431, 0.04246104,\n",
       "        0.05245849, 0.05087268, 0.0445761 , 0.06453408, 0.06264225,\n",
       "        0.05533205, 0.06434057, 0.04361894, 0.03823989, 0.05176992,\n",
       "        0.05144846, 0.04938575, 0.05948372, 0.04943102, 0.04693404,\n",
       "        0.05439546, 0.05216075, 0.04083384, 0.04963371, 0.04710256,\n",
       "        0.03336942, 0.06332203, 0.05183794, 0.04171809, 0.04857362,\n",
       "        0.04594921, 0.04255765, 0.05444399, 0.0417523 , 0.03935366,\n",
       "        0.03935172, 0.0495708 , 0.04146004, 0.04274684, 0.05152798,\n",
       "        0.04450407, 0.04398401, 0.05180568, 0.05239356, 0.03869535,\n",
       "        0.05427629, 0.05372103, 0.03494889, 0.05068547, 0.04965832,\n",
       "        0.03446874, 0.05995587, 0.05119433, 0.05692732, 0.0535461 ,\n",
       "        0.05268854, 0.05412272, 0.04952217, 0.04278299, 0.04231923,\n",
       "        0.05576307, 0.04587463, 0.05502193, 0.05545975, 0.04920663,\n",
       "        0.06404778, 0.05354081, 0.04940963, 0.04891942, 0.05245051,\n",
       "        0.04398141, 0.06026391, 0.05065318, 0.05122161, 0.06115906,\n",
       "        0.04168308, 0.0461709 , 0.04719052, 0.05387559, 0.0380083 ,\n",
       "        0.0387105 , 0.03843047, 0.03287785, 0.05730716, 0.04698021,\n",
       "        0.051845  , 0.04673031, 0.04747127, 0.05369581, 0.04587757,\n",
       "        0.04880264, 0.04590226, 0.04670411, 0.04304487, 0.05020125,\n",
       "        0.0492211 , 0.0518866 , 0.06234154, 0.04876816, 0.05921006,\n",
       "        0.05599441, 0.05017324, 0.05216032, 0.05724539, 0.0498344 ,\n",
       "        0.05631358, 0.05909398, 0.05578056, 0.06386929, 0.05048987,\n",
       "        0.06272338, 0.06556465, 0.05656317, 0.04355977, 0.05108064,\n",
       "        0.05947658, 0.0532833 , 0.05735701, 0.04954849, 0.05976983,\n",
       "        0.062939  , 0.05325115, 0.0477797 , 0.05196764, 0.05494501,\n",
       "        0.04569593, 0.05266616, 0.04951622, 0.04975833, 0.0546839 ,\n",
       "        0.04747507, 0.0450791 , 0.04979544, 0.04467616, 0.04484343,\n",
       "        0.04796101, 0.05256453, 0.05212687, 0.0552884 , 0.04681725,\n",
       "        0.04090497, 0.05457818, 0.04317195, 0.04857181, 0.05181057,\n",
       "        0.056115  , 0.05281594, 0.05233868, 0.04977414, 0.03517592,\n",
       "        0.05347302, 0.044551  , 0.04412023, 0.04740625, 0.05488853,\n",
       "        0.05862601, 0.05559632, 0.04862756, 0.05230754, 0.05319284,\n",
       "        0.05167531, 0.05520563, 0.05262435, 0.05596161, 0.05833895,\n",
       "        0.05448051, 0.04907981, 0.04567519, 0.05439382, 0.05087884,\n",
       "        0.03342125, 0.04671311, 0.05035899, 0.03967073, 0.04845098,\n",
       "        0.04556326, 0.04921518, 0.05113133, 0.0476812 , 0.04344504,\n",
       "        0.04752719, 0.04006319, 0.05145488, 0.05407431, 0.04246104,\n",
       "        0.05245849, 0.05087268, 0.0445761 , 0.06453408, 0.06264225,\n",
       "        0.05533205, 0.06434057, 0.04361894, 0.03823989, 0.05176992,\n",
       "        0.05144846, 0.04938575, 0.05948372, 0.04943102, 0.04693404,\n",
       "        0.05439546, 0.05216075, 0.04083384, 0.04963371, 0.04710256,\n",
       "        0.03336942, 0.06332203, 0.05183794, 0.04171809, 0.04857362,\n",
       "        0.04594921, 0.04255765, 0.05444399, 0.0417523 , 0.03935366,\n",
       "        0.03935172, 0.0495708 , 0.04146004, 0.04274684, 0.05152798,\n",
       "        0.04450407, 0.04398401, 0.05180568, 0.05239356, 0.03869535,\n",
       "        0.05427629, 0.05372103, 0.03494889, 0.05068547, 0.04965832,\n",
       "        0.03446874, 0.05995587, 0.05119433, 0.05692732, 0.0535461 ,\n",
       "        0.05268854, 0.05412272, 0.04952217, 0.04278299, 0.04231923,\n",
       "        0.05576307, 0.04587463, 0.05502193, 0.05545975, 0.04920663,\n",
       "        0.06404778, 0.05354081, 0.04940963, 0.04891942, 0.05245051,\n",
       "        0.04398141, 0.06026391, 0.05065318, 0.05122161, 0.06115906,\n",
       "        0.04168308, 0.0461709 , 0.04719052, 0.05387559, 0.0380083 ,\n",
       "        0.0387105 , 0.03843047, 0.03287785, 0.05730716, 0.04698021,\n",
       "        0.051845  , 0.04673031, 0.04747127, 0.05369581, 0.04587757,\n",
       "        0.04880264, 0.04590226, 0.04670411, 0.04304487, 0.05020125,\n",
       "        0.0492211 , 0.0518866 , 0.06234154, 0.04876816, 0.05921006,\n",
       "        0.05599441, 0.05017324, 0.05216032, 0.05724539, 0.0498344 ,\n",
       "        0.05631358, 0.05909398, 0.05578056, 0.06386929, 0.05048987,\n",
       "        0.06272338, 0.06556465, 0.05656317, 0.04355977, 0.05108064,\n",
       "        0.05947658, 0.0532833 , 0.05735701, 0.04954849, 0.05976983,\n",
       "        0.062939  , 0.05325115, 0.0477797 , 0.05196764, 0.05494501,\n",
       "        0.04569593, 0.05266616, 0.04951622, 0.04975833, 0.0546839 ,\n",
       "        0.04747507, 0.0450791 , 0.04979544, 0.04467616, 0.04484343,\n",
       "        0.04796101, 0.05256453, 0.05212687, 0.0552884 , 0.04681725,\n",
       "        0.04090497, 0.05457818, 0.04317195, 0.04857181, 0.05181057,\n",
       "        0.056115  , 0.05281594, 0.05233868, 0.04977414, 0.03517592,\n",
       "        0.05347302, 0.044551  , 0.04412023, 0.04740625, 0.05488853,\n",
       "        0.05862601, 0.05559632, 0.04862756, 0.05230754, 0.05319284,\n",
       "        0.05167531, 0.05520563, 0.05262435, 0.05596161, 0.05833895,\n",
       "        0.05448051, 0.04907981, 0.04567519, 0.05439382, 0.05087884,\n",
       "        0.03816129, 0.03518592, 0.04075258, 0.04914624, 0.04776831,\n",
       "        0.04941648, 0.04775401, 0.06341077, 0.05496931, 0.04958089,\n",
       "        0.04605519, 0.04897918, 0.04973751, 0.04176968, 0.04776757,\n",
       "        0.04459878, 0.06036219, 0.05310885, 0.05729076, 0.04813312,\n",
       "        0.04245504, 0.05436377, 0.0447545 , 0.04784204, 0.04860457,\n",
       "        0.05297003, 0.05570314, 0.0567386 , 0.04155167, 0.04368582,\n",
       "        0.05584914, 0.04322847, 0.03903681, 0.05219343, 0.05232356,\n",
       "        0.03616148, 0.06237612, 0.04188921, 0.03925779, 0.05371559,\n",
       "        0.0483467 , 0.04854556, 0.06032737, 0.05695128, 0.04513514,\n",
       "        0.04627783, 0.03721314, 0.0337867 , 0.05666147, 0.04448453,\n",
       "        0.05136025, 0.05026203, 0.04815016, 0.05320229, 0.05318021,\n",
       "        0.04538596, 0.04926359, 0.05646089, 0.05034035, 0.04468789,\n",
       "        0.04070753, 0.05619731, 0.05453729, 0.05031366, 0.04939743,\n",
       "        0.0605145 , 0.04866982, 0.0467345 , 0.04456927, 0.04236491,\n",
       "        0.04909074, 0.05427463, 0.05603018, 0.04673237, 0.04422357,\n",
       "        0.05431653, 0.04320362, 0.04282084, 0.04508954, 0.05897238,\n",
       "        0.05370971, 0.06357209, 0.0424579 , 0.0479248 , 0.05721302,\n",
       "        0.05175175, 0.04258794, 0.05030686, 0.06074379, 0.05184144,\n",
       "        0.05082291, 0.02993929, 0.03830383, 0.06216933, 0.04570842,\n",
       "        0.04792113, 0.04321683, 0.04409874, 0.04765355, 0.04998556,\n",
       "        0.04986715, 0.04487354, 0.05259948, 0.04792679, 0.04964042,\n",
       "        0.03950308, 0.0538643 , 0.05204479, 0.0531972 , 0.0538901 ,\n",
       "        0.05058668, 0.05720173, 0.04793628, 0.05494283, 0.03815762,\n",
       "        0.04771691, 0.05787219, 0.05644809, 0.04616912, 0.04741514,\n",
       "        0.05184756, 0.05616092, 0.0484463 , 0.04391026, 0.05364264,\n",
       "        0.05802794, 0.05058097, 0.04882764, 0.04886972, 0.05763106,\n",
       "        0.05447267, 0.05389363, 0.04186004, 0.05448764, 0.05680812,\n",
       "        0.0597256 , 0.05028869, 0.04754808, 0.06223935, 0.05289919,\n",
       "        0.05121671, 0.05800556, 0.04804207, 0.0533574 , 0.05196918,\n",
       "        0.0513209 , 0.05642011, 0.06001586, 0.05174424, 0.05536379,\n",
       "        0.05345758, 0.03865305, 0.04059513, 0.06837951, 0.05382093,\n",
       "        0.05107438, 0.05943977, 0.0527892 , 0.05202275, 0.04393994,\n",
       "        0.0376682 , 0.0427533 , 0.05519104, 0.04892661, 0.04920166,\n",
       "        0.0504471 , 0.04843146, 0.05236624, 0.0429899 , 0.0297756 ,\n",
       "        0.03491666, 0.056032  , 0.05486682, 0.04772114, 0.052434  ,\n",
       "        0.04923666, 0.05087227, 0.04760869, 0.0398153 , 0.04798134]),\n",
       " 'rank_test_score': array([366,  80, 154, 128,  35,  74, 167, 105,  16, 315,  82, 174, 335,\n",
       "        233, 276, 324,  61,  11,  84,  59,   9, 163, 226,  27, 223,   3,\n",
       "         21, 103,  41, 118,  45,  47,  51,  67, 177,   1, 238,  90, 342,\n",
       "        107, 152, 132, 156,  76,  57, 431, 124,  14, 359, 159,  25, 302,\n",
       "        327, 180, 421, 144, 126, 427, 274, 236, 373, 200, 114, 406,  99,\n",
       "         88, 411, 191, 449, 293, 165, 202, 184,  54,  95, 347, 142, 198,\n",
       "        266, 161,  86, 244,  69,  97, 188, 116,  39, 254, 109,  30, 401,\n",
       "        194,  23, 468, 268, 146, 454, 250, 337, 439, 288,   5, 456, 169,\n",
       "         72, 458, 379, 272, 295, 171,  92, 433, 149, 196, 349, 240, 218,\n",
       "        228, 134,  32, 394,  49,  19, 333, 204, 252, 210, 247,   7, 331,\n",
       "         65, 101, 307, 182, 291, 506, 477, 377, 518, 484, 381, 526, 465,\n",
       "        447, 504, 399, 321, 502, 460, 368, 516, 415, 361, 392, 363, 305,\n",
       "        429, 311, 284, 509, 413, 221, 417, 318, 186, 313, 442, 256, 488,\n",
       "        387, 263, 279, 403, 309, 356, 375, 136, 473, 345, 140, 366,  80,\n",
       "        154, 128,  35,  74, 167, 105,  16, 315,  82, 174, 335, 233, 276,\n",
       "        324,  61,  11,  84,  59,   9, 163, 226,  27, 223,   3,  21, 103,\n",
       "         41, 118,  45,  47,  51,  67, 177,   1, 238,  90, 342, 107, 152,\n",
       "        132, 156,  76,  57, 431, 124,  14, 359, 159,  25, 302, 327, 180,\n",
       "        421, 144, 126, 427, 274, 236, 373, 200, 114, 406,  99,  88, 411,\n",
       "        191, 449, 293, 165, 202, 184,  54,  95, 347, 142, 198, 266, 161,\n",
       "         86, 244,  69,  97, 188, 116,  39, 254, 109,  30, 401, 194,  23,\n",
       "        468, 268, 146, 454, 250, 337, 439, 288,   5, 456, 169,  72, 458,\n",
       "        379, 272, 295, 171,  92, 433, 149, 196, 349, 240, 218, 228, 134,\n",
       "         32, 394,  49,  19, 333, 204, 252, 210, 247,   7, 331,  65, 101,\n",
       "        307, 182, 291, 506, 477, 377, 518, 484, 381, 526, 465, 447, 504,\n",
       "        399, 321, 502, 460, 368, 516, 415, 361, 392, 363, 305, 429, 311,\n",
       "        284, 509, 413, 221, 417, 318, 186, 313, 442, 256, 488, 387, 263,\n",
       "        279, 403, 309, 356, 375, 136, 473, 345, 140, 355,  53,  64, 453,\n",
       "        173, 123, 398, 271, 259, 423,  37, 287, 426, 179, 214, 389, 298,\n",
       "         94, 339, 232, 111, 358, 231, 326, 419, 215,  43, 216, 212, 386,\n",
       "         38, 320, 176, 299, 148,  13, 353,  79, 390,  34, 260, 130, 151,\n",
       "        209,  56, 493,  78,  18, 441, 112, 208, 408, 323, 230, 462, 249,\n",
       "         63, 471, 190, 262, 420, 384, 317, 476, 286, 139, 464, 206, 282,\n",
       "        446, 372, 330, 425, 122, 207, 424, 258, 283, 370, 351, 304, 467,\n",
       "        217, 138, 131, 329, 242, 397, 225, 301, 481, 270, 243, 497, 341,\n",
       "         71, 495, 290, 300, 508, 352, 113, 515, 344, 220, 528, 383, 281,\n",
       "        496, 391,  44, 491, 278, 193, 523, 445, 213, 475, 365, 121, 480,\n",
       "        340, 158, 498, 410, 235, 463, 261,  29, 472, 265, 120, 483, 354,\n",
       "        246, 537, 490, 499, 539, 487, 470, 538, 482, 438, 536, 520, 452,\n",
       "        535, 511, 435, 540, 525, 437, 524, 492, 436, 521, 501, 444, 534,\n",
       "        514, 371, 522, 494, 451, 530, 500, 409, 533, 512, 385, 529, 479,\n",
       "        396, 531, 486, 405, 532, 513, 297], dtype=int32)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907208606102268"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.775785\n",
      "F1: 0.652778\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a24189b50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 231,\n",
       " 'Parch': 91,\n",
       " 'male': 42,\n",
       " 'Pclass': 81,\n",
       " 'SibSp': 86,\n",
       " 'S': 43,\n",
       " 'Q': 37,\n",
       " 'youngin': 27,\n",
       " 'Age': 150}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n",
      "              learning_rate=0.07, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=3, missing=nan, n_estimators=500, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "205.797px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
